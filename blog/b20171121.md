# Spark's Configuration
## Configuring Spark with SparkConf
A `SparkConf` instance is required when you are creating a new `SparkContext`.
- Scala code
```scala
val conf = new SparkConf()
conf.set("spark.app.name", "My Spark App")
conf.set("spark.master", "local[*]")

val sc = new SparkContext(conf)
```
- Python code
```python
conf = new SparkConf()
conf.set("spark.app.name", "My Spark App")
conf.set("spark.master", "local")

sc = SparkContext(conf)
```
- Java code
```java
SparkConf conf = new SparkConf();
conf.set("spark.app.name", "My Spark App");
conf.set("spark.master", "local");

JavaSparkContext sc = JavaSparkContext(conf);
```

## Configuring Spark using flags
```
$ spark-submit --class com.example.App --master local[2] --name "My Spark App" --conf spark.ui.port=36000 myApp.jar
$ spark-submit --class com.example.App --properties-file my-config.conf myApp.jar

## Contents of my-config.conf ##
spark.master local[2]
spark.app.name "My Spark App"
spark.ui.port 36000
```

## Configuration Precedence Order
The highest priority is given to configurations using the `set()` function on a `SparkConf` object. Next are flags passed to `spark-submit`. Then values in the properties file, and finally default values.

## Visualizing RDDs with toDebugString()
```
scala> input.toDebugString
res0: String =
(2) input.txt MapPartitionsRDD[1] at textFile at <console>:24 []
 |  input.txt HadoopRDD[0] at textFile at <console>:24 []
 scala> counts.toDebugString
 res1: String =
(2) ShuffledRDD[5] at reduceByKey at <console>:28 []
 +-(2) MapPartitionsRDD[4] at map at <console>:28 []
    |  MapPartitionsRDD[3] at filter at <console>:26 []
    |  MapPartitionsRDD[2] at map at <console>:26 []
    |  input.txt MapPartitionsRDD[1] at textFile at <console>:24 []
    |  input.txt HadoopRDD[0] at textFile at <console>:24 []
```

## Phases of Spark Execution
1. User code defines a Directed Acyclic Graph of RDDs
2. Actions force translation of the DAG to an execution plan
3. Tasks are scheduled and executed on a cluster

## Finding Information
- Spark Web UI
- Logs: For the Spark's Standalone mode and Mesos, logs are displayed in master's web UI, and stored in the work/ directory of each worker. For the Yarn mode, the easiest way to collect logs of a finished application is to run `yarn logs -applicationId <app ID>`. For viewing logs of a running application in Yarn, you can click through the ResourceManager UI to the Nodes page. You can specify the logging level by setting some properties in the `conf/log4j.properties` file.

## Parallelism
Spark offers two ways to tune the degree of parallelism for operations: 
- give a degree of parallelism as a parameter during operations that shuffle data;
- use `repartition()` operator to randomly shuffle an RDD into the desired number of partitions; or use `coalesce()` operator to shrink the RDD partitions.  
Let's see an example in python.
```
>>> input = sc.textFile("s3n://log-files/2014/*.log")
>>> input.getNumPartitions()
100
>>> input = input.coalesce(2).cache()
>>> lines.getNumPartitions()
2
```

## Serialization
- By default, Java's built-in serializer
- Kryo serializer 
```
val conf = new SparkConf()
conf.set("spark.serializer", "org.apache.spark.serializer.kryoSerializer")
```

## Memory Management
Tuning Spark's use of memory can help optimize your application. Memory is used for:
- RDD storage   
When you call `persist()` or `cache()` on an RDD, its partitions will be stored in memory buffers. You can limit the amount of memory used when caching by setting `spark.storage.memoryFraction`.
-Shuffle and aggregation buffers  
When performing shuffle operations, Spark will create intermediate buffers for storing shuffle output data. You can limit the total amount of memory used in shuffle-related buffers by setting `spark.shuffle.memoryFraction`.
-User code  
User code has access to everything left in the JVM heap after the space for RDD storage and shuffle storage are allocated.  
By default Spark will leave 60% of space for RDD storage, 20% for shuffle memory, and the remaining 20% for user programs.

## Hardware
- executor memory  
Set `spark.executor.memory` or `--executor-memory` flag to `spark-submit`.
- number of executor cores
In Yarn, set `spark.executor.cores` or `--executor-cores` flag and `--num-executors` flag to determine the total count.  
In Mesos and Standalone mode, set `spark.cores.max` to limit the total number of cores across all executors.
- local disks for intermediate data  
In Yarn, the configuration is read directly from Yarn.
In Standalone mode, set the `SPARK_LCOAL_DIRS` environment variable in `conf/spark-env.sh`.  
In Mesos mode, set the `spark.local.dir` option.
