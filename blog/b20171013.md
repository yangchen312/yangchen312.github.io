# Hadoop Related Projects
## Avro
  A language-independent data serialization system
  * Install Avro for Python
    ```
    $ easy_install avro
    ```

  * Avro tools for Java

    You can download Avro tools written in Java directly from the website:
    http://www.apache.org/dyn/closer.cgi/avro/.

## Parquet
  A columnar storage format that can efficiently store nested data
  * Download Parquet tools

    You can download Parquet tools from the Parquet Maven repository. Search for "parquet-tools" on http://seach.maven.org
    * Get the metadata of a Parquet file
    ```
    $ java -jar parquet-tools-*.jar meta data.parquet
    ```

## Flume
  It's designed for high-volume ingestion into Hadoop of even-based (streaming) data.
  * Flume agent

    To use Flume, you need to run a Flume agent, which is a long-lived Java that runs sources and sinks (data destination, e.g. HDFS), connected by channels. A basic Flume building block is a **source-channel-sink** combination.

  * Installing Flume
    1. Download Flume binary release from the website: https://flume.apache.org/download.html.
    2. Move/copy and unpack the archive file in a suitable location, here it is `/usr/local`.
    ```
    $ sudo cp apache-flume-1.8.0-bin.tar.gz /usr/local/
    $ cd /usr/local
    $ sudo tar -xzvf apache-flume-1.8.0-bin.tar.gz
    $ sudo mv apache-flume-1.8.0-bin flume
    ```
    3. Put the Flume binary on your path by adding below lines into `~/.profile` or `/.bashrc`.
    ```
    $ sudo nano ~/.bashrc
    ```
    Append below lines in `~/.bashrc`:
    ```
    export FLUME_HOME=/usr/local/flume
    export FLUME_CONF_DIR=$FLUME_HOME/conf
    export FLUME_CLASSPATH=$FLUME_CONF_DIR
    export PATH=$PATH:$FLUME_HOME/bin
    ```
    4. Instantiate `flume-env.sh` and `flume-conf.properties`
    ```
    $ sudo cp conf/flume-env.sh.template conf/flume-env.sh
    $ sudo cp conf/flume-conf.properties.template conf/spool-to-hdfs.properties
    ```
    Add the following lines in `flume-env.sh`.
    ```
    export JAVA_HOME="$(/usr/libexec/java_home)"
    ```

  * An example
    - Firstly, specify the cofiguration file. Here is an example.
    ```
    # conf/spool-to-hdfs.properties

    # Name components
    agent1.sources = source1
    agent1.channels = channel1
    agent1.sinks = sink1

    # Bind source and sink to channel
    agent1.sources.source1.channels = channel1
    agent1.sinks.sink1.channel = channel1

    # Describe source
    agent1.sources.source1.type = spooldir
    agent1.sources.source1.spoolDir = /tmp/spooldir

    # Describe sink
    agent1.sinks.sink1.type = hdfs
    agent1.sinks.sink1.hdfs.path = /tmp/flume
    agent1.sinks.sink1.hdfs.filePrefix = events
    agent1.sinks.sink1.hdfs.fileSuffix = .log
    agent1.sinks.sink1.hdfs.inUsePrefix = _
    agent1.sinks.sink1.hdfs.fileType = DataStream

    # Describe channel
    agent1.channels.channel1.type = file
    ```
    - Create the spooling directory
    ```
    $ sudo mkdir /tmp/spooldir
    $ sudo chown -R hadoop_user:group_name /tmp/spooldir/
    ```
    - Start the Flume agent
    ```
    $ flume-ng agent --conf-file conf/spool-to-hdfs.properties --name agent1 --conf $FLUME_HOME/conf -Dflume.root.logger=INFO,console
    ```
    - In a new terminal, create a file in the spooling directory.
    ```
    $ echo -e "Hello\nFlume" > /tmp/spooldir/.file1.txt
    $ mv /tmp/spooldir/.file1.txt /tmp/spooldir/file1.txt
    ```
    - In the agent's terminal, you will see that Flume is processing the file.
    ```
    Preparing to move file /tmp/spooldir/file1.txt to /tmp/spooldir/file1.txt.COMPLETED
    ...
    Renaming /tmp/flume/_events.1508227733764.log.tmp to /tmp/flume/events.1508227733764.log
    ```
      The file was renamed to `file1.txt.COMPLETED` by the source, which indicates that Flume won't process it again.
    - Check event file in the HDFS.
    ```
    $ hadoop fs -cat /tmp/flume/events.1508227733764.log
    Hello
    Flume
    ```
    - To stop Flume agents, use the `kill` command.

    Firstly, determine the PID number related to the process for each agent.
    ```
    $ ps â€“ef | grep flume
    ```
    Secondly, stop the process related to the Flume agent.
    ```
    $ kill -9 process_ID
    ```

## Sqoop
A tool that allows users to transfer data between Hadoop and structured data stores for further processing. You can use Sqoop to import data from a RDBMS into HDFS, transform data into Hadoop MapReduce, and then export data back into a RDBMS.
* Installing Sqoop
  1. Download Sqoop binary release from the website http://www.apache.org/dyn/closer.lua/sqoop/1.4.6.
  2. Move the archieve file into `/usr/local`, unpack and rename it.
  ```
  $ sudo cp sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz /usr/local
  $ cd /usr/local
  $ sudo tar -xvf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz
  $ sudo mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha sqoop
  ```
  3. Add Sqoop binary on your path.
  ```
  $ sudo nano ~/.bashrc
  ```
    Append the following lines into `~/.bashrc`.
  ```
  export SQOOP_HOME=/usr/local/sqoop
  export PATH=$PATH:$SQOOP_HOME/bin
  ```
  4. Configuring Sqoop
  ```
  $ cd /usr/local/sqoop/conf
  $ sudo cp sqoop-env-template.sh sqoop-env.sh
  ```
    Open sqoop-env.sh and edit the following lines:
  ```
  export HADOOP_COMMON_HOME=/usr/local/Cellar/hadoop/2.8.1/libexec
  export HADOOP_MAPRED_HOME=/usr/local/Cellar/hadoop/2.8.1/libexec
  ```
  5. To verify whether Sqoop is properly installed:
  ```
  $ sqoop help
  ```

* Sqoop import
  > To import a MySQL table as text files into HDFS:

  1. Make sure the Hadoop user can access to the MySQL database. Here is an example.
  ```
  mysql> GRANT ALL PRIVILEGES ON database_name.* TO ''@'hostname';
  ```
  2. Download the JDBC driver JAR file for MySQL (Connector/J) from the website https://dev.mysql.com/downloads/connector/j/5.1.html, and add it to Sqoop's classpath, which is simply achieved by placing it in Sqoop's `lib` directory.
  3. import a MySQL table into HDFS using following command:
  ```
  $ sqoop import --connect jdbc:mysql://hostname/database_name --table table_name -m 1
  ```
  Here Sqoop use a single map task by specifying `-m 1`, so we get a single file in HDFS. By default, Sqoop's import tool run a MapResult job and use **4** map tasks in parallel, and you will get **4** files in a common directory.

  4. You can inspect this file content using the following command.
  ```
  $ hadoop fs -cat database_name/part-m-00000
  ```

  > To import a MySQL table as `SequenceFiles` into HDFS.

  Other steps are similar to the "text files" import, except the third step:
  ```
  $ sqoop import --connect jdbc:mysql://localhost/database_name --table table_name -m 1 --class-name HolderName --as-sequencefile --target-dir sequence_files_name --bindir .
  ```
  The **CLASS** and **JAR** files named "HolderName" is generated in the current directory (with `--bindir`), and we can reuse it for export the `sequence_files_name` to a MySQL table.


* Sqoop export
  > To export records stored in `SequenceFiles` to a MySQL table:

  ```
  $ sqoop export --connect jdbc:mysql://localhost/database_name --table new_table_name -m 1 --class-name HolderName --jar-file HolderName.jar --export-dir sequence_files_name
  ```
  The columns of the destination MySQL table should be created before exporting.

## Pig
A platform for processing large datasets. It's made up of two pieces:
1. Pig Latin: the language used to express data flows.
2. Execution modes to run Pig Latin programs:
> Local mode: Pig runs in a single JVM and accesses the local filesystem. It's suitable for small datasets.
> MapReduce mode:  Pig translates queries into MapReduce jobs and runs them on a Hadoop cluster.
  You can use `-x` or `exectype` option to set the execution mode and start **Grunt**, the Pig interactive shell.

  Local mode:
  ```
  $ pig -x local
  ```
  MapReduce mode:
  ```
  $ pig
  or
  $ pig -x mapreduce
  ```
  If you've finished your Grunt session, you can exit with the `quit` or `\q` command.

* Installing Pig
  1. Download a pig stable release from http://pig.apache.org/releases.html, unpack the tarball in `/usr/local`, and rename it.
  2. Add Pig's binary directory to your command-line path, and set the JAVA_HOME environment variable. For example:
  ```
  $ export SQOOP_HOME=/usr/local/sqoop
  $ export PATH=$PATH:$SQOOP_HOME/bin
  $ export JAVA_HOME="$(/usr/libexec/java_home)"
  ```
  3. Inspect whether it's properly installed by typing `pig -version`.

* Pig Latin
  - An example
  ```
  -- max_temp.pig: finds the maximum temperature by year
  records = LOAD 'input/ncdc/micro-tab/sample.txt' AS (year:chararray, temperature:int, quality:int);
  filtered_records = filter records by temperature != 9999 and quality in (0,1,4,5,9);
  grouped_records = group filtered_records by year;
  max_temp = foreach grouped_records generate group, max(filtered_records.temperature);
  dump max_temp;
  ```
  Pig uses a tuple to represent a row of data in a database table, and a relation given a name or alias (like records) is a set of tuples. You can examine the contents of an alias using `DUMP` operator.
  ```
  grunt> DUMP records;
  ```
  You can also see the *schema* of a relation using the `DESCRIBE` operator.
  ```
  grunt> DESCRIBE records;
  ```
  You can also see the logical and physical plans created by Pig using `EXPLAIN` command on a relation.
  ```
  grunt> EXPLAIN max_temp;
  ```

  - Comments
  Pig Latin has two types of comments: one is `--` (double hyphens), the other one is `/*` and `*/` markers (C-style comments).
