# Hadoop Related Projects
## Avro
  A language-independent data serialization system
  * Install Avro for Python
    ```
    $ easy_install avro
    ```

  * Avro tools for Java

    You can download Avro tools written in Java directly from the website:
    http://www.apache.org/dyn/closer.cgi/avro/.

## Parquet
  A columnar storage format that can efficiently store nested data
  * Download Parquet tools

    You can download Parquet tools from the Parquet Maven repository. Search for "parquet-tools" on http://seach.maven.org
    * Get the metadata of a Parquet file
    ```
    $ java -jar parquet-tools-*.jar meta data.parquet
    ```

## Flume
  It's designed for high-volume ingestion into Hadoop of even-based (streaming) data.
  * Flume agent

    To use Flume, you need to run a Flume agent, which is a long-lived Java that runs sources and sinks (data destination, e.g. HDFS), connected by channels. A basic Flume building block is a **source-channel-sink** combination.

  * Installing Flume
    1. Download Flume binary release from the website: https://flume.apache.org/download.html.
    2. Move/copy and unpack the archive file in a suitable location, here it is `/usr/local`.
    ```
    $ sudo cp apache-flume-1.8.0-bin.tar.gz /usr/local/
    $ cd /usr/local
    $ sudo tar -xzvf apache-flume-1.8.0-bin.tar.gz
    $ sudo mv apache-flume-1.8.0-bin flume
    ```
    3. Put the Flume binary on your path by adding below lines into `~/.profile` or `/.bashrc`.
    ```
    $ sudo nano ~/.bashrc
    ```
    Append below lines in `~/.bashrc`:
    ```
    export FLUME_HOME=/usr/local/flume
    export FLUME_CONF_DIR=$FLUME_HOME/conf
    export FLUME_CLASSPATH=$FLUME_CONF_DIR
    export PATH=$PATH:$FLUME_HOME/bin
    ```
    4. Instantiate `flume-env.sh` and `flume-conf.properties`
    ```
    $ sudo cp conf/flume-env.sh.template conf/flume-env.sh
    $ sudo cp conf/flume-conf.properties.template conf/spool-to-hdfs.properties
    ```
    Add the following lines in `flume-env.sh`.
    ```
    export JAVA_HOME="$(/usr/libexec/java_home)"
    ```

  * An example
    - Firstly, specify the cofiguration file. Here is an example.
    ```
    # conf/spool-to-hdfs.properties

    # Name components
    agent1.sources = source1
    agent1.channels = channel1
    agent1.sinks = sink1

    # Bind source and sink to channel
    agent1.sources.source1.channels = channel1
    agent1.sinks.sink1.channel = channel1

    # Describe source
    agent1.sources.source1.type = spooldir
    agent1.sources.source1.spoolDir = /tmp/spooldir

    # Describe sink
    agent1.sinks.sink1.type = hdfs
    agent1.sinks.sink1.hdfs.path = /tmp/flume
    agent1.sinks.sink1.hdfs.filePrefix = events
    agent1.sinks.sink1.hdfs.fileSuffix = .log
    agent1.sinks.sink1.hdfs.inUsePrefix = _
    agent1.sinks.sink1.hdfs.fileType = DataStream

    # Describe channel
    agent1.channels.channel1.type = file
    ```
    - Create the spooling directory
    ```
    $ sudo mkdir /tmp/spooldir
    $ sudo chown -R hadoop_user:group_name /tmp/spooldir/
    ```
    - Start the Flume agent
    ```
    $ flume-ng agent --conf-file conf/spool-to-hdfs.properties --name agent1 --conf $FLUME_HOME/conf -Dflume.root.logger=INFO,console
    ```
    - In a new terminal, create a file in the spooling directory.
    ```
    $ echo -e "Hello\nFlume" > /tmp/spooldir/.file1.txt
    $ mv /tmp/spooldir/.file1.txt /tmp/spooldir/file1.txt
    ```
    - In the agent's terminal, you will see that Flume is processing the file.
    ```
    Preparing to move file /tmp/spooldir/file1.txt to /tmp/spooldir/file1.txt.COMPLETED
    ...
    Renaming /tmp/flume/_events.1508227733764.log.tmp to /tmp/flume/events.1508227733764.log
    ```
      The file was renamed to `file1.txt.COMPLETED` by the source, which indicates that Flume won't process it again.
    - Check event file in the HDFS.
    ```
    $ hadoop fs -cat /tmp/flume/events.1508227733764.log
    Hello
    Flume
    ```
    - To stop Flume agents, use the `kill` command.

    Firstly, determine the PID number related to the process for each agent.
    ```
    $ ps â€“ef | grep flume
    ```
    Secondly, stop the process related to the Flume agent.
    ```
    $ kill -9 process_ID
    ```

## Sqoop
  A tool that allows users to transfer data between Hadoop and structured data stores for further processing. You can use Sqoop to import data from a RDBMS into HDFS, transform data into Hadoop MapReduce, and then export data back into a RDBMS.
  * Installing Sqoop
    1. Download Sqoop binary release from the website http://www.apache.org/dyn/closer.lua/sqoop/1.4.6.
    2. Move the archieve file into `/usr/local`, unpack and rename it.
    ```
    $ sudo cp sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz /usr/local
    $ cd /usr/local
    $ sudo tar -xvf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz
    $ sudo mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha sqoop
    ```
    3. Add Sqoop binary on your path.
    ```
    $ sudo nano ~/.bashrc
    ```
    Append the following lines into `~/.bashrc`.
    ```
    export SQOOP_HOME=/usr/local/sqoop
    export PATH=$PATH:$SQOOP_HOME/bin
    ```
    4. Configuring Sqoop
    ```
    $ cd /usr/local/sqoop/conf
    $ sudo cp sqoop-env-template.sh sqoop-env.sh
    ```
    Open sqoop-env.sh and edit the following lines:
    ```
    export HADOOP_COMMON_HOME=/usr/local/Cellar/hadoop/2.8.1/libexec
    export HADOOP_MAPRED_HOME=/usr/local/Cellar/hadoop/2.8.1/libexec
    ```
    5. To verify whether Sqoop is properly installed:
    ```
    $ sqoop help
    ```
