# Hadoop Related Projects
## Avro
  A language-independent data serialization system
  * Install Avro for Python
    ```
    $ easy_install avro
    ```

  * Avro tools for Java

    You can download Avro tools written in Java directly from the website:
    http://www.apache.org/dyn/closer.cgi/avro/.

## Parquet
  A columnar storage format that can efficiently store nested data
  * Download Parquet tools

    You can download Parquet tools from the Parquet Maven repository. Search for "parquet-tools" on http://seach.maven.org
    * Get the metadata of a Parquet file
    ```
    $ java -jar parquet-tools-*.jar meta data.parquet
    ```

## Flume
  It's designed for high-volume ingestion into Hadoop of even-based (streaming) data.
  * Flume agent

    To use Flume, you need to run a Flume agent, which is a long-lived Java that runs sources and sinks (data destination, e.g. HDFS), connected by channels. A basic Flume building block is a **source-channel-sink** combination.

  * Installing Flume
    1. Download Flume binary release from the website: https://flume.apache.org/download.html.
    2. Move/copy and unpack the archive file in a suitable location, here it is `/usr/local`.
    ```
    $ sudo cp apache-flume-1.8.0-bin.tar.gz /usr/local/
    $ cd /usr/local
    $ sudo tar -xzvf apache-flume-1.8.0-bin.tar.gz
    $ sudo mv apache-flume-1.8.0-bin flume
    ```
    3. Put the Flume binary on your path by adding below lines into `~/.profile` or `/.bashrc`.
    ```
    $ sudo nano ~/.bashrc
    ```
    Append below lines in `~/.bashrc`:
    ```
    export FLUME_HOME=/usr/local/flume
    export FLUME_CONF_DIR=$FLUME_HOME/conf
    export FLUME_CLASSPATH=$FLUME_CONF_DIR
    export PATH=$PATH:$FLUME_HOME/bin
    ```
    4. Instantiate `flume-env.sh` and `flume-conf.properties`
    ```
    $ sudo cp conf/flume-env.sh.template conf/flume-env.sh
    $ sudo cp conf/flume-conf.properties.template conf/spool-to-hdfs.properties
    ```
    Add the following lines in `flume-env.sh`.
    ```
    export JAVA_HOME="$(/usr/libexec/java_home)"
    ```

  * An example
    - Firstly, specify the cofiguration file. Here is an example.
    ```
    # conf/spool-to-hdfs.properties

    # Name components
    agent1.sources = source1
    agent1.channels = channel1
    agent1.sinks = sink1

    # Bind source and sink to channel
    agent1.sources.source1.channels = channel1
    agent1.sinks.sink1.channel = channel1

    # Describe source
    agent1.sources.source1.type = spooldir
    agent1.sources.source1.spoolDir = /tmp/spooldir

    # Describe sink
    agent1.sinks.sink1.type = hdfs
    agent1.sinks.sink1.hdfs.path = /tmp/flume
    agent1.sinks.sink1.hdfs.filePrefix = events
    agent1.sinks.sink1.hdfs.fileSuffix = .log
    agent1.sinks.sink1.hdfs.inUsePrefix = _
    agent1.sinks.sink1.hdfs.fileType = DataStream

    # Describe channel
    agent1.channels.channel1.type = file
    ```
    - Create the spooling directory
    ```
    $ sudo mkdir /tmp/spooldir
    $ sudo chown -R hadoop_user:group_name /tmp/spooldir/
    ```
    - Start the Flume agent
    ```
    $ flume-ng agent --conf-file conf/spool-to-hdfs.properties --name agent1 --conf $FLUME_HOME/conf -Dflume.root.logger=INFO,console
    ```
    - In a new terminal, create a file in the spooling directory.
    ```
    $ echo -e "Hello\nFlume" > /tmp/spooldir/.file1.txt
    $ mv /tmp/spooldir/.file1.txt /tmp/spooldir/file1.txt
    ```
    - In the agent's terminal, you will see that Flume is processing the file.
    ```
    Preparing to move file /tmp/spooldir/file1.txt to /tmp/spooldir/file1.txt.COMPLETED
    ...
    Renaming /tmp/flume/_events.1508227733764.log.tmp to /tmp/flume/events.1508227733764.log
    ```
      The file was renamed to `file1.txt.COMPLETED` by the source, which indicates that Flume won't process it again.
    - Check event file in the HDFS.
    ```
    $ hadoop fs -cat /tmp/flume/events.1508227733764.log
    Hello
    Flume
    ```
    - To stop Flume agents, use the `kill` command.

    Firstly, determine the PID number related to the process for each agent.
    ```
    $ ps â€“ef | grep flume
    ```
    Secondly, stop the process related to the Flume agent.
    ```
    $ kill -9 process_ID
    ```

## Sqoop
  A tool that allows users to transfer data between Hadoop and structured data stores for further processing. You can use Sqoop to import data from a RDBMS into HDFS, transform data into Hadoop MapReduce, and then export data back into a RDBMS.
  * Installing Sqoop
    1. Download Sqoop binary release from the website http://www.apache.org/dyn/closer.lua/sqoop/1.4.6.
    2. Move the archieve file into `/usr/local`, unpack and rename it.
    ```
    $ sudo cp sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz /usr/local
    $ cd /usr/local
    $ sudo tar -xvf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz
    $ sudo mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha sqoop
    ```
    3. Add Sqoop binary on your path.
    ```
    $ sudo nano ~/.bashrc
    ```
    Append the following lines into `~/.bashrc`.
    ```
    export SQOOP_HOME=/usr/local/sqoop
    export PATH=$PATH:$SQOOP_HOME/bin
    ```
    4. Configuring Sqoop
    ```
    $ cd /usr/local/sqoop/conf
    $ sudo cp sqoop-env-template.sh sqoop-env.sh
    ```
    Open sqoop-env.sh and edit the following lines:
    ```
    export HADOOP_COMMON_HOME=/usr/local/Cellar/hadoop/2.8.1/libexec
    export HADOOP_MAPRED_HOME=/usr/local/Cellar/hadoop/2.8.1/libexec
    ```
    5. To verify whether Sqoop is properly installed:
    ```
    $ sqoop help
    ```

  * Sqoop import
  - To import a MySQL table as text files into HDFS:
  1. Make sure the Hadoop user can access to the MySQL database. Here is an example.
  ```
  mysql> GRANT ALL PRIVILEGES ON database_name.* TO ''@'hostname';
  ```
  2. Download the JDBC driver JAR file for MySQL (Connector/J) from the website https://dev.mysql.com/downloads/connector/j/5.1.html, and add it to Sqoop's classpath, which is simply achieved by placing it in Sqoop's `lib` directory.
  3. import a MySQL table into HDFS using following command:
  ```
  $ sqoop import --connect jdbc:mysql://hostname/database_name --table table_name -m 1
  ```
  Here Sqoop use a single map task by specifying `-m 1`, so we get a single file in HDFS. By default, Sqoop's import tool run a MapResult job and use *4* map tasks in parallel, and you will get *4* files in a common directory.
  4. You can inspect this file content.
  ```
  $ hadoop fs -cat database_name/part-m-00000
  ```

  - To import a MySQL table as `SequenceFiles` into HDFS.
  Other steps are similar to the "text files" import, except the third step:
  ```
  $ sqoop import --connect jdbc:mysql://localhost/database_name --table table_name -m 1 --class-name HolderName --as-sequencefile --target-dir sequence_files_name --bindir .
  ```
  The *CLASS* and *JAR* files named "HolderName" is generated in the current directory (with `--bindir`), and we can reuse it for export the `sequence_files_name` to a MySQL table.


  * Sqoop export
  - To export records stored in `SequenceFiles` to a MySQL table:
  ```
  $ sqoop export --connect jdbc:mysql://localhost/database_name --table new_table_name -m 1 --class-name HolderName --jar-file HolderName.jar --export-dir sequence_files_name
  ```
  The columns of the destination MySQL table should be created before exporting.

## Pig
  A platform for processing large datasets. It's made up of two pieces:
  1. Pig Latin: the language used to express data flows.
  2. Execution modes to run Pig Latin programs:
    - Local mode: Pig runs in a single JVM and accesses the local filesystem. It's suitable for small datasets.
    - MapReduce mode:  Pig translates queries into MapReduce jobs and runs them on a Hadoop cluster.
    You can use `-x` or `exectype` option to set the execution mode and start **Grunt**, the Pig interactive shell.
    Local mode:
    ```
    $ pig -x local
    ```
    MapReduce mode:
    ```
    $ pig
    or
    $ pig -x mapreduce
    ```
    If you've finished your Grunt session, you can exit with the `quit` or `\q` command.

  * Installing Pig
    1. Download a pig stable release from http://pig.apache.org/releases.html, unpack the tarball in `/usr/local`, and rename it.
    2. Add Pig's binary directory to your command-line path, and set the JAVA_HOME environment variable. For example:
    ```
    $ export SQOOP_HOME=/usr/local/sqoop
    $ export PATH=$PATH:$SQOOP_HOME/bin
    $ export JAVA_HOME="$(/usr/libexec/java_home)"
    ```
    3. Inspect whether it's properly installed by typing `pig -version`.

  * Pig Latin
  - An example
  ```
  -- max_temp.pig: finds the maximum temperature by year
  records = LOAD 'input/ncdc/micro-tab/sample.txt' AS (year:chararray, temperature:int, quality:int);
  filtered_records = filter records by temperature != 9999 and quality in (0,1,4,5,9);
  grouped_records = group filtered_records by year;
  max_temp = foreach grouped_records generate group, max(filtered_records.temperature);
  dump max_temp;
  ```
  Pig uses a tuple to represent a row of data in a database table, and a relation given a name or alias (like records) is a set of tuples. You can examine the contents of an alias using `DUMP` operator.
  ```
  grunt> DUMP records;
  ```
  You can also see the *schema* of a relation using the `DESCRIBE` operator.
  ```
  grunt> DESCRIBE records;
  ```
  You can also see the logical and physical plans created by Pig using `EXPLAIN` command on a relation.
  ```
  grunt> EXPLAIN max_temp;
  ```

  - Comments
  Pig Latin has two types of comments: one is `--` (double hyphens), the other one is `/*` and `*/` markers (C-style comments).

## Hive
  Hive converts your SQL query into a series of jobs for execution on a Hadoop cluster. Hive organizes data into tables, which provides a means for attaching structure to data stored in HDFS.

  * Installing Hive
    1. Download a release from the website https://hive.apache.org/downloads.html.
    2. Unpack the tarball in `/usr/local` and rename it.
    ```
    $ sudo tar -xzf apache-hive-2.3.0-bin.tar.gz
    $ sudo mv apache-hive-2.3.0-bin hive
    ```
    3. Open `~/.bashrc`, and put Hive on your path.
    ```
    $ export HIVE_HOME=/usr/local/hive
    $ export PATH=$PATH:$HIVE_HOME/bin
    ```
    4. To configure Hive:
    ```
    $ cd /usr/local/hive
    $ sudo cp conf/hive-env.sh.template conf/hive-env.sh
    $ sudo nano conf/hive-env.sh
    ```
    Append the following lines:
    ```
    export HADOOP_HOME=/usr/local/Cellar/hadoop/2.8.1/libexec
    export HIVE_CONF_DIR=/usr/local/hive/conf
    ```
    5. Create Hive directories within HDFS. The directory `warehouse` is the location to store the table or data related to hive. And set read/write permissions for table.
    ```
    $ hadoop fs -mkdir -p /users/hive/warehouse
    $ hadoop fs -mkdir /tmp
    $ hadoop fs -chmod g+w /users/hive/warehouse
    $ hadoop fs -chmod g+w /tmp
    ```
    6. To configure Metastore of Hive: to specify where the database is stored.
    ```
    $ sudo cp conf/hive-default.xml.template conf/hive-site.xml
    ```
    Edit `hive-site.xml` and find these properties and make sure their values are as follows.
    ```xml
    <property>
      <name>javax.jdo.option.ConnectionURL</name>
      <value>jdbc:derby:;databaseName=/usr/local/hive/metastore_db;create=true</value>
      <description>JDBC connect string for a JDBC metastore.</description>
    </property>
    <property>
      <name>hive.metastore.warehouse.dir</name>
      <value>/user/hive/warehouse</value>
      <description>location of default database for the warehouse</description>
    </property>
    ```
    If an error `java.io.FileNotFoundException: derby.log (Permission denied)` appears, which means Hive needs read and write permissions on its own folders, you can issue the following command:
    ```
    $ sudo chmod -R ugo+rw /usr/local/hive
    ```
    7. By default, Hive uses **Derby** database. Initialize Derby database with `schematool`, which is located in `$HIVE_HOME/bin`:
    ```
    $ schematool -initSchema -dbType derby
    Metastore connection URL:	 jdbc:derby:;databaseName=metastore_db;create=true
    Metastore Connection Driver :	 org.apache.derby.jdbc.EmbeddedDriver
    Metastore connection User:	 APP
    Starting metastore schema initialization to 2.3.0
    Initialization script hive-schema-2.3.0.derby.sql
    Initialization script completed
    schemaTool completed
    ```
    A metastore_db directory will be generated in `/usr/local/hive`, due to the setting of `javax.jdo.option.ConnectionURL`.
    8. Launch the Hive CLI (Command-line interface):
    ```
    $ hive
    hive> show databases;
    OK
    default
    Time taken: 6.177 seconds, Fetched: 1 row(s)
    ```
    A file `derby.log` will be generated in `/usr/local/hive`, due to the setting of `javax.jdo.option.ConnectionURL`.
    If you find an error `Caused by: java.net.URISyntaxException: Relative path in absolute URI: ${system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D`, you can add the following lines at the beginning of `hive-site.xml`.
    ```xml
    <property>
      <name>system:java.io.tmpdir</name>
      <value>/tmp/hive/java</value>
    </property>
    <property>
      <name>system:user.name</name>
      <value>${user.name}</value>
    </property>
    ```
    9. You can leave the shell using the statements `quit;` or `exit;`.

  * Installing Derby
  The installation is similar to Hive.
    1. Download a release from the website https://db.apache.org/derby/derby_downloads.html.
    2. Unpack the tarball in `/usr/local`, and rename it as `derby`.

  * Common operations of Hive
    - Inspect or locate databases and tables.
    ```
    hive> show databases;
    hive> use database_name;
    hive> show tables;
    hive> select * from table_name;
    ```
    - Create a table and import data from a local file.
    ```
    hive> create table records (year string, temperature int, quality int) row format delimited fields terminated by '\t';
    hive> load data local inpath 'input/ncdc/micro-tab/sample.txt' overwrite into table records;
    ```
    - Import data from a relational database (e.g. **MySQL**) using **Sqoop**.
    
