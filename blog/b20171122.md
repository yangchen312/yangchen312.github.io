# Get Started with Spark SQL
Spark SQL provides a special type of RDDs called **SchemaRDDs**, which are similar to tables in a traditional database. A SchemaRDD is an RDD composed of Row objects with additional schema information of the types in each column.

## Initializing Spark SQL
To use Spark SQL inside a Spark application, we construct a HiveContext or SQLContext based on our SparkContext.
- Spark SQL Imports in Scala
```scala
// Import Spark SQL
import org.apache.spark.sql.hive.HiveContext
// Or if you can't have the hive dependencies
import org.apache.spark.sql.SQLContext
// Create a Spark SQL HiveContext
val hiveCtx = new HiveContext(sc)
// Import the implicit conversions
import hiveCtx._
```

- Spark SQL Imports in Python
```python
from pyspark.sql import HiveContext, Row
from pyspark.sql import SQLContext, Row
hiveCtx = HiveContext(sc)
```
**Debugging:** when you launch the `pyspark` shell, if the "pyspark.sql.utils.IllegalArgumentException: u"Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':"" exception occurs, you should eliminate the Hadoop related configuration, e.g. HADOOP_CONF_DIR.

- Spark SQL Imports in Java
```java
import org.apache.spark.api.java.JavaSparkContext
import org.apache.spark.sql.hive.HiveContext;
import org.apache.spark.sql.SQLContext;
import org.apache.spark.sql.SchemaRDD;
import org.apache.spark.sql.Row;

JavaSparkContext ctx = new JavaSparkContext();
SQLContext sqlCtx = new HiveContext(ctx);
```

## Basic Query
- Load a JSON file and give a SQL query
1. In Scala
```scala
val tweets = hiveCtx.read.json("testweet.json")
// Registered as a "temporary table"
tweets.registerTempTable("tweets")
val topTweets = hiveCtx.sql("SELECT text, retweetCount FROM tweets ORDER BY retweetCount LIMIT 10")
```

2. In Python
```python
tweets = hiveCtx.read.json("/path/to/tweets.json")
tweets.registerTempTable("tweets")
topTweets = hiveCtx.sql("SELECT text, retweetCount FROM tweets ORDER BY retweetCount LIMIT 10")
```

3. In Java
SchemaRDD tweets = sqlCtx.read.json("testweet.json");
tweets.registerTempTable("tweets");
SchemaRDD topTweets = sqlCtx.sql("SELECT text, retweetCount FROM tweets ORDER BY retweetCount LIMIT 10");

- Select a column from a SchemaRDD
1. In Scala
```scala
val topTweetText = topTweets.map(row => row.getString(0))  // getString(0) would return field 0 as a string
```

2. In Python
```python
topTweetText = topTweets.map(lambda row: row.text)  //Python Rows support named access to their fields, row.column_name
```

3. In Java
```
JavaRDD<String> topTweetText = topTweets.toJavaRDD().map(new Function<Row, String>() {
	public String call(Row row) {
		return row.getString(0);
	}
});
```

- Cache a table
```
hiveCtx.cacheTable("tableName")
```
The cached table will remain in memory only for the life of our driver program. You can also cache tables using SQL statements. This is commonly used with command-line clients to the JDBC server. e.g.
```
CACHE TABLE tableName
UNCACHE TABLE tableName
```

## Loading and Saving Data
The previous examples have shown loading JSON data to Hive, and how to load Hive table to SchemaRDD. Here we will show how to load data from other sources (i.e. Parquet, other RDDs).

- Parquet
It's a popular column-oriented storage format used in the Hadoop ecosystem. It supports all of the data types in Spark SQL. Spark SQL provides methods for reading data to and from Parquet files.
```python
# Parquet load in Python
rows = hiveCtx.parquetFile(parquetFile)
names = rows.map(lambda row: row.name)
names.collect()
# Parquet query
tbl = rows.registerTempTable("people")
pandaFriends = hiveCtx.sql("SELECT name FROM people WHERE favouriteAnimal = \"panda\"")
print pandaFriends.map(lambda row: row.name).collect()
# Save the SchemaRDD to Parquet
pandaFriends.saveAsParquetFile("hdfs:///path")
```

## JDBC/ODBC Server 
Spark SQL also provides JDBC connectivity, and the Spark SQL ODBC driver is produced by Simba, and can be downloaded from various Spark vendors. They are useful for connecting BI tools to a Spark cluster and for sharing a cluster across multiple users. 
- Launching the JDBC server  
Spark SQL's JDBC server corresponds to the HiveServer2 in Hive. It's also known as "Thrift server" since it uses the Thrift communication protocol.
```
$ cd SPARK_HOME
$ ./sbin/start-thriftserver.sh
```
By default it listens on localhost:10000, but these can be changed with environment variables, e.g. HIVE_SERVER@_THRIFT_PORT.

- Connecting to JDBC server with Beeline  
Spark ships with the Beeline client program that we can use to connect to the JDBC server.
```
$ ./bin/beeline -u jdbc:hive2://localhost:10000
Connecting to jdbc:hive2://localhost:10000
Connected to: Spark SQL (version 2.2.1)
Driver: Hive JDBC (version 1.2.1.spark2)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Beeline version 1.2.1.spark2 by Apache Hive
0: jdbc:hive2://localhost:10000> show tables;
+-----------+------------+--------------+--+
| database  | tableName  | isTemporary  |
+-----------+------------+--------------+--+
+-----------+------------+--------------+--+
No rows selected (0.899 seconds)
```


I










