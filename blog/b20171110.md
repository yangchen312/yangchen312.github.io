# Spark in Java
## An Simple Example
### Including Spark API in a Java Project
1. Create a Java project using Maven. You can find how to implement it in [my another post](../b20171023).  
2. Append the following lines in the `pom.xml` file.  
```xml
<dependency>
	<groupId>org.apache.spark</groupId>
	<artifactId>spark-sql_2.11</artifactId>
	<version>2.2.0</version>
</dependency>
```

### Application
```java
package examples;

import org.apache.spark.api.java.function.FilterFunction;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.SparkSession;

public class SimpleApp {
    public static void main(String[] args) {
        String logFile = "/usr/local/spark/README.md";
        SparkSession spark = SparkSession.builder().appName("Simple Application").master("local").getOrCreate();
        Dataset<String> logData = spark.read().textFile(logFile).cache();

        long numAs = logData.filter((FilterFunction<String>) s -> s.contains("a")).count();
        long numBs = logData.filter((FilterFunction<String>) s -> s.contains("b")).count();

        System.out.println("Lines with a: " + numAs + ", lines with b: " + numBs);

        spark.stop();
    }
}
```
If you meet a language level problem, like `Error:java: javacTask: source release 1.8 requires target release 1.8`, you should append the following lines in the `pom.xml`.  
```xml
<build>
    <plugins>
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-compiler-plugin</artifactId>
        <configuration>
          <source>1.8</source>
          <target>1.8</target>
        </configuration>
      </plugin>
    </plugins>
  </build>
```
You can run the application in IDE or package the application using Maven, and run it using `bin/spark-submit`.
```
$ mvn package
$ spark-submit --class "examples.SimpleApp" target/simple-project-1.0-SNAPSHOT.jar
Lines with a: 61, lines with b: 30
```


