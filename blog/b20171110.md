# Spark in Java
## An Simple Example
### Including Spark API in a Java Project
1. Create a Java project using Maven. You can find how to implement it in [my another post](../blog/b20171023).  
2. Append the following lines in the `pom.xml` file. Please note that Spark artifacts are tagged with a Scala version.
```xml
<dependency>
	<groupId>org.apache.spark</groupId>
	<artifactId>spark-sql_2.11</artifactId>
	<version>2.2.0</version>
</dependency>
```

### An Application Example
```java
package examples;

import org.apache.spark.api.java.function.FilterFunction;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.SparkSession;

public class SimpleApp {
    public static void main(String[] args) {
        String logFile = "/usr/local/spark/README.md";
        SparkSession spark = SparkSession.builder().appName("Simple Application").master("local").getOrCreate();
        Dataset<String> logData = spark.read().textFile(logFile).cache();

        long numAs = logData.filter((FilterFunction<String>) s -> s.contains("a")).count();
        long numBs = logData.filter((FilterFunction<String>) s -> s.contains("b")).count();

        System.out.println("Lines with a: " + numAs + ", lines with b: " + numBs);

        spark.stop();
    }
}
```
If you meet a language level problem, like `Error:java: javacTask: source release 1.8 requires target release 1.8`, you should append the following lines in the `pom.xml`.  
```xml
<build>
    <plugins>
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-compiler-plugin</artifactId>
        <configuration>
          <source>1.8</source>
          <target>1.8</target>
        </configuration>
      </plugin>
    </plugins>
  </build>
```
You can run the application in IDE or package the application using Maven, and run it using `bin/spark-submit`.
```
$ mvn package
$ spark-submit --class "examples.SimpleApp" target/simple-project-1.0-SNAPSHOT.jar
Lines with a: 61, lines with b: 30
```

### Initializing Spark in Java
```java
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaSparkContext;

SparkConf conf = new SparkConf().setMaster("local").setAppName("My App");
JavaSparkContext sc = new JavaSparkContext(conf);
```

## Programming on RDDs
### Creating RDDs
```
JavaRDD<String> lines = sc.parallelize(Arrays.asList("pandas", "i like pandas"));
JavaRDD<String> lines = sc.textFile("/path/to/README.md");
```

### Transformations
filter(), map(), flatMap() transformations
```java
JavaRDD<String> inputRDD = sc.textFile("log.txt");
JavaRDD<String> errorRDD = inputRDD.filter(
	new Function<String, Boolean>() {
		public Boolean call(String x) {return x.contains("error");}
	}
);
JavaRDD<Integer> result = rdd.map(new Function<Integer, Integer>() {
	public Integer call(Integer x) {return x * x;}
});
JavaRDD<String> words = lines.flatMap(new FlatMapFunction<String, String>() {
	public Iterable<String> call(String line) {
		return Arrays.asList(line.split(" "));
	}
});
```

### Actions
`take(k)`: returning k elements, reduce()
```
for (String line: badLineRDD.take(10)) {
	System.out.println(line);
}
Integer sum = rdd.reduce(new Function2<Integer, Integer, Integer> () {
	public Integer call(Integer x, Integer y) {return x + y;}
})
```

## Operations for Key/Value Pairs
- Creating a pair RDD
```
PairFunction<String, String, String> keyData =
	new PairFunction<String, String, String>() {
		public Tuple2<String, String> call(String x) {
			return new Tuple2(x.split(" ")[0], x);
		}
	};
JavaPairRDD<String, String> pairs = lines.mapToPair(keyData);
```



