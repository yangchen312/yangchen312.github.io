# Spark in Python
## PySpark shell Operations
Due to Python's dynamic nature, the Dataset to be strongly-typed isn't needed in Python. All Datasets in Python are Dataset[Row], and we call it `DataFrame` to be consistent with the data frame concept in Pandas.  

### Create a DataFrame from a file in local filesystem.
```
>>> textFile = spark.read.text("/usr/local/spark/README.md")
```

### Transformations and Actions on DataFrame
1. Common actions  
```
>>> testFile.collect()
>>> textFile.count()
>>> textFile.first()
```

2. Filtering
```
>>> textFile.filter(textFile.value.contains("Spark")).count()
```

3. MapReduce flows
```
>>> from pyspark.sql.functions import *
>>> textFile.select(size(split(textFile.value, "\s+")).name("numWords")).agg(max(col("numWords"))).collect()
[Row(max(numWords)=22)]
>>> wordCounts = textFile.select(explode(split(textFile.value, "\s+")).name("word")).groupBy("word").count()
>>> wordCounts.collect()
```

## Import PySpark into PyCharm
### Adding `SPARK_HOME` and `PYTHONPATH` to the run configuration's environment variables.  
1. Direct to `Run` -> `Edit Configurations`.
2. Edit `Environment Variables`, add two new variables, i.e. `SPARK_HOME=/usr/local/spark`, `PYTHONPATH=/usr/local/spark/python/lib`.
3. Click OK.

### Adding the `$SPARK_HOME/python` directory and `$SPARK_HOME/python/lib/py4j-*-src.zip` to the project interpreter's classpath.
1. Go to `PyCharm` -> `Preferences` -> `Project` -> `Project Interpreter`.
2. Click Project Interpreter dropdown button, and click `Show All`.
3. On the Project Interpreters window, click the `Show Paths for the selected interpreter` button.
4. Add `$SPARK_HOME/python` and `$SPARK_HOME/python/lib/py4j-*-src.zip` into the Interpreter Paths.
5. Click OK.

### Verifying your Pycharm is correctly configured for PySpark. Create a `test.py` file in the project with the following lines, and run it.
```python
try:
    from operator import add
    from pyspark import SparkContext
    print ("Successfully imported Spark Modules.")

except ImportError as e:
    print ("Cannot import Spark modules", e)
```

## Self-Contained Applications
### An Simple Example
```python
"""SimpleApp.py"""
from pyspark.sql import SparkSession

logFile = "/usr/local/spark/README.md"
spark = SparkSession.builder.appName("Simple Example").getOrCreate()
logData = spark.read.text(logFile).cache()

numAs = logData.filter(logData.value.contains('a')).count()
numBs = logData.filter(logData.value.contains('b')).count()

print("Lines with a: %i, lines with b: %i" % (numAs, numBs))

spark.stop()
```
You can run this application using `spark-submit`.
```
$ spark-submit SimpleApp.py
...
Lines with a: 61, lines with b: 30
```






