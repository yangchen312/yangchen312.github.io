# Spark in Python
## PySpark shell Operations
Due to Python's dynamic nature, the Dataset to be strongly-typed isn't needed in Python. All Datasets in Python are Dataset[Row], and we call it `DataFrame` to be consistent with the data frame concept in Pandas.  

### Create a DataFrame from a file in local filesystem.
```
>>> textFile = spark.read.text("/usr/local/spark/README.md")
>>> lines = sc.textFile("/usr/local/spark/README.md")
>>> type(textFile)
<class 'pyspark.rdd.RDD'>
>>> type(lines)
<class 'pyspark.sql.dataframe.DataFrame'>
```

### Transformations and Actions on DataFrame
1. Common actions  
```
>>> testFile.collect()
>>> textFile.count()
>>> textFile.first()
```

2. Filtering
```
>>> textFile.filter(textFile.value.contains("Spark")).count()
```

3. MapReduce flows
```
>>> from pyspark.sql.functions import *
>>> textFile.select(size(split(textFile.value, "\s+")).name("numWords")).agg(max(col("numWords"))).collect()
[Row(max(numWords)=22)]
>>> wordCounts = textFile.select(explode(split(textFile.value, "\s+")).name("word")).groupBy("word").count()
>>> wordCounts.collect()
```

## Import PySpark into PyCharm
### Adding `SPARK_HOME` and `PYTHONPATH` to the run configuration's environment variables.  
1. Direct to `Run` -> `Edit Configurations`.
2. Edit `Environment Variables`, add two new variables, i.e. `SPARK_HOME=/usr/local/spark`, `PYTHONPATH=/usr/local/spark/python/lib`.
3. Click OK.

### Adding the `$SPARK_HOME/python` directory and `$SPARK_HOME/python/lib/py4j-*-src.zip` to the project interpreter's classpath.
1. Go to `PyCharm` -> `Preferences` -> `Project` -> `Project Interpreter`.
2. Click Project Interpreter dropdown button, and click `Show All`.
3. On the Project Interpreters window, click the `Show Paths for the selected interpreter` button.
4. Add `$SPARK_HOME/python` and `$SPARK_HOME/python/lib/py4j-*-src.zip` into the Interpreter Paths.
5. Click OK.

### Verifying your Pycharm is correctly configured for PySpark. Create a `test.py` file in the project with the following lines, and run it.
```python
try:
    from operator import add
    from pyspark import SparkContext
    print ("Successfully imported Spark Modules.")

except ImportError as e:
    print ("Cannot import Spark modules", e)
```

## Import PySpark into Jupyter
There are two methods to use PySpark in a Jupyter Notebook or shell:  
### Method-1: Configuring PySpark driver
Configure PySpark driver to use Jupyter Notebook.
Append the following lines to the `~/.bashrc` or `~/.profile` file to open Jupyter Notebook.
```
export PYSPARK_DRIVER_PYTHON=jupyter
export PYSPARK_DRIVER_PYTHON_OPTS='notebook'
```
Or you can just put the configuration as arguments of the `pyspark` command.
```
$ PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook' pyspark
```
Use the following configuration to open Ipython Shell.
```
export PYSPARK_DRIVER_PYTHON=ipython
export PYSPARK_DRIVER_PYTHON_OPTS=''
```
Running `pyspark` will automatically open a Jupyter notebook.

### Method-2: Loading Pyspark using FindSpark package
Open a Jupyter Notebook and load PySpark using findSpark package. Firstly, you should install `findspark`:
```
$ pip install findspark
```
Then launch a Jupyter notebook, and write the following script:
```python
import findspark
findspark.init()

import pyspark
```

## Standalone Applications
### An Simple Example
```python
"""SimpleApp.py"""
from pyspark.sql import SparkSession

logFile = "/usr/local/spark/README.md"
spark = SparkSession.builder.appName("Simple Example").getOrCreate()
logData = spark.read.text(logFile).cache()

numAs = logData.filter(logData.value.contains('a')).count()
numBs = logData.filter(logData.value.contains('b')).count()

print("Lines with a: %i, lines with b: %i" % (numAs, numBs))

spark.stop()
```
You can run this application using `spark-submit`.
```
$ spark-submit SimpleApp.py
...
Lines with a: 61, lines with b: 30
```

### Initializing Spark in Python
```python
from pyspark import SparkConf, SparkContext

conf = SparkConf().setMaster("local").setAppName("My App")
sc = SparkContext(conf = conf)
```

## Programming on RDDs
### Creating RDDs
```
lines = sc.parallelize(["pandas", "i like pandas"])
lines = sc.textFile("/path/to/README.md")
```
### Transformations
filter(), map(), flatMap() transformations
```
inputRDD = sc.textFile("log.txt")
errorsRDD = inputRDD.filter(lambda x: "error" in x)
squared = nums.map(lambda x: x * x).collect()
words = lines.flatMap(lambda line: line.split(" "))
```

### Actions
`take(k)`: returning k elements, reduce(), aggregate(), collect()
```
for line in badLineRDD.take(10):
	print line
sum = rdd.reduce(lambda x, y: x + y)
sumCount = nums.aggregate((0, 0), 
				(lambda acc, value: (acc[0] + value, acc[1] + 1)),
				(lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])))
return sumCount[0] / float(sumCount[1])
```

## Operations for Key/Value Pairs
- Creating a pair RDD
```
pairs = lines.map(lambda x: (x.split(" ")[0], x))
```





