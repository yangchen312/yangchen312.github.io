## Getting Started with Hive
  Hive converts your SQL query into a series of jobs for execution on a Hadoop cluster. Hive organizes data into tables, which provides a means for attaching structure to data stored in HDFS.

  * Installing Hive
    1. Download a release from the website https://hive.apache.org/downloads.html.
    2. Unpack the tarball in `/usr/local` and rename it.
    ```
    $ sudo tar -xzf apache-hive-2.3.0-bin.tar.gz
    $ sudo mv apache-hive-2.3.0-bin hive
    ```
    3. Open `~/.bashrc`, and put Hive on your path.
    ```
    $ export HIVE_HOME=/usr/local/hive
    $ export PATH=$PATH:$HIVE_HOME/bin
    ```
    4. To configure Hive:
    ```
    $ cd /usr/local/hive
    $ sudo cp conf/hive-env.sh.template conf/hive-env.sh
    $ sudo nano conf/hive-env.sh
    ```
    Append the following lines:
    ```
    export HADOOP_HOME=/usr/local/Cellar/hadoop/2.8.1/libexec
    export HIVE_CONF_DIR=/usr/local/hive/conf
    ```
    5. Create Hive directories within HDFS. The directory `warehouse` is the location to store the table or data related to hive. And set read/write permissions for table.
    ```
    $ hadoop fs -mkdir -p /users/hive/warehouse
    $ hadoop fs -mkdir /tmp
    $ hadoop fs -chmod g+w /users/hive/warehouse
    $ hadoop fs -chmod g+w /tmp
    ```
    6. To configure Metastore of Hive: to specify where the database is stored.
    ```
    $ sudo cp conf/hive-default.xml.template conf/hive-site.xml
    ```
    Edit `hive-site.xml` and find these properties and make sure their values are as follows.
    ```xml
    <property>
      <name>javax.jdo.option.ConnectionURL</name>
      <value>jdbc:derby:;databaseName=/usr/local/hive/metastore_db;create=true</value>
      <description>JDBC connect string for a JDBC metastore.</description>
    </property>
    <property>
      <name>hive.metastore.warehouse.dir</name>
      <value>/user/hive/warehouse</value>
      <description>location of default database for the warehouse</description>
    </property>
    ```
    If an error `java.io.FileNotFoundException: derby.log (Permission denied)` appears, which means Hive needs read and write permissions on its own folders, you can issue the following command:
    ```
    $ sudo chmod -R ugo+rw /usr/local/hive
    ```
    7. By default, Hive uses **Derby** database. Initialize Derby database with `schematool`, which is located in `$HIVE_HOME/bin`:
    ```
    $ schematool -initSchema -dbType derby
    Metastore connection URL:	 jdbc:derby:;databaseName=metastore_db;create=true
    Metastore Connection Driver :	 org.apache.derby.jdbc.EmbeddedDriver
    Metastore connection User:	 APP
    Starting metastore schema initialization to 2.3.0
    Initialization script hive-schema-2.3.0.derby.sql
    Initialization script completed
    schemaTool completed
    ```
    A `metastore_db` directory will be generated in `/usr/local/hive`, due to the setting of `javax.jdo.option.ConnectionURL`.
    8. Launch the Hive CLI (Command-line interface):
    ```
    $ hive
    hive> show databases;
    OK
    default
    Time taken: 6.177 seconds, Fetched: 1 row(s)
    ```
    A file `derby.log` will be generated in `/usr/local/hive`, due to the setting of `javax.jdo.option.ConnectionURL`.  
    If you'd like to re-initialize hive metastore, you can delete the `metastore_db` directory and `derby.log` file in your `$HIVE_HOME`, and rerun the schemaTool.  
    If you find an error `Caused by: java.net.URISyntaxException: Relative path in absolute URI: ${system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D`, you can add the following lines at the beginning of `hive-site.xml`.
    ```xml
    <property>
      <name>system:java.io.tmpdir</name>
      <value>/tmp/hive/java</value>
    </property>
    <property>
      <name>system:user.name</name>
      <value>${user.name}</value>
    </property>
    ```
    9. You can leave the shell using the statements `quit;` or `exit;`.

  * Installing Derby  
    The installation is similar to Hive.
    1. Download a release from the website https://db.apache.org/derby/derby_downloads.html.
    2. Unpack the tarball in `/usr/local`, and rename it as `derby`.

  * Changing Hive metastore from Derby to MySQL  
    Prerequisites:  
    You should install and configure MySQL database server well.
    Step-1:  
    Download the JDBC driver JAR file for MySQL (Connector/J) from the website https://dev.mysql.com/downloads/connector/j/5.1.html, and put the JAR file in Hive's `lib` directory.
    Step-2:  
    Start MySQL server and access MySQL CLI, and create a new database **metastore** for hive.
    ```
    $ mysql -u root -p
    mysql> CREATE DATABASE metastore;
    ```  
    Step-3:  
    Create a MySQL user for Hive to access metastore.
    ```
    mysql> CREATE USER username@hostname IDENTIFIED BY 'user_password';
    mysql> GRANT ALL PRIVILEGES ON metastore.* TO username@hostname IDENTIFIED BY 'user_password';
    mysql> FLUSH PRIVILEGES;
    ```  
    Step-4:  
    You need to configure Hive to access MySQL metastore. In this case, you can reset some properties in `hive-site.xml` file.  
    ```xml
    <configuration>
      <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://hostname/metastore?createDatabaseIfNotExist=true</value>
        <description>metadata is stored in a MySQL server</description>
      </property>
      <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
        <description>MySQL JDBC driver class</description>
      </property>
      <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>username</value>
        <description>user name for connecting to MySQL server</description>
      </property>
      <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>user_password</value>
        <description>password for connecting to MySQL server</description>
      </property>
    </configuration>
    ```  
    Step-5:
    Run the Hive schematool to initialize MySQL metastore. If it is not on your path, you can find it in `$HIVE_HOME/bin`.  
    ```
    $ schematool -initSchema -dbType mysql
    ...
    Metastore connection URL:	 jdbc:mysql://localhost/metastore?createDatabaseIfNotExist=true
    Metastore Connection Driver :	 com.mysql.jdbc.Driver
    Metastore connection User:	 yangchen
    ...
    schemaTool completed
    ```
    Step-6:
    Start Hive and create a table and insert one record.
    ```
    hive> show databases;
    hive> create table consumers(id int, name string);
    hive> show tables;
    hive> insert into consumers values (1, "Alice");
    ...
    Loading data to table default.consumers
    MapReduce Jobs Launched:
    Stage-Stage-1: Map: 1   HDFS Read: 4190 HDFS Write: 81 SUCCESS
    ...
    hive> select * from customers;
    OK
    1	Alice
    Time taken: 0.164 seconds, Fetched: 1 row(s)
    ```
    Then access MySQL, go to metastore database, and you'll see your table is a record in TBLS table of metastore database.  
    ```
    $ mysql -u root -p
    mysql> use metastore;
    mysql> show tables;
    mysql> select * from TBLS;
    ```



  * Common operations of Hive
    - Inspect, locate or drop databases and tables.
    ```
    hive> show databases;
    hive> use database_name;
    hive> show tables;
    hive> select * from table_name;
    hive> drop table table_name;
    ```
    - Create a table and import data from a local file.
    ```
    hive> create table records (year string, temperature int, quality int) row format delimited fields terminated by '\t';
    hive> load data local inpath 'input/ncdc/micro-tab/sample.txt' overwrite into table records;
    ```
    - Create a table from a relational database (e.g. **MySQL**) using **Sqoop**.  
    Sqoop can generate a Hive table based on a table from an existing relational database.
    ```
    $ sqoop create-hive-table --connect jdbc:mysql://localhost/hadoopguide --table widgets --fields-terminated-by ','
    ```  
    - Import data from a relational database (e.g. **MySQL**) using **Sqoop**.
    ```
    $ sqoop import --connect jdbc:mysql://localhost/hadoopguide --table widgets -m 1 --hive-import
    ```  
    - Import data from HDFS into a Hive table
    ```
    hive> LOAD DATA INPATH "/HDFS_path/widgets" INTO TABLE widgets;
    ```
    - Populate a Hive table with data from another Hive table.  
    You can use the construct `CREATE/INSERT TABLE ... [AS] SELECT`. for example,
    ```
    hive> CREATE TABLE target AS SELECT col1, col2 FROM source; 
    ```
