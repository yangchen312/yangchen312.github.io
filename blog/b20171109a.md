# Spark in Scala
Spark's primary abstraction is a distributed collection of Datasets. Before Spark 2.0, the main programming interface of Spark was the **Resilient Distributed Dataset (RDD)**. After Spark 2.0, RDDs are replaced by Datasets. 

## Spark-shell Operations
### Create a new Dataset from a file in the local filesystem.
```
scala> val textFile = spark.read.textFile("file_path/file_name")

```

### Transformations and Actions on Datasets
1. Common actions
```
scala> textFile.count()
scala> textFile.first()
scala> textFile.collect()
```

2. Filtering  
```
scala> textFile.filter(line => line.contains("Spark"))
```

3. MapReduce flows
```
scala> textFile.map(line => line.split(" ").size).reduce((a, b) => if (a > b) a else b)
scala> val wordCounts = textFile.flatMap(line => line.split(" ")).groupByKey(identity).count()
```

4. Caching
```
scala> textFile.cache()
```

## Standalone Applications
### An Simple Example
You can build an application using the Spark API in Scala with sbt. You can find how to create an application using sbt in [my another post](../b20171102.md). After you have created an application module,  
firstly, you should include Spark as a dependency in the sbt configuration file `build.sbt`. See the example,  
```
name := "quickstart"
version := "0.1"
scalaVersion := "2.11.8"
libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.2.0"
```
There probably are errors caused by version conflicts, like `sbt.librarymanagement.ResolveException: unresolved dependency: org.apache.spark#spark-sql_2.12;2.2.0: not found`. The problem may be that Spark is written with Scala of another version. You should specify that version.  
Secondly, you can create the application in Scala, here it's named `SimpleApp.scala` in the directory `src/main/scala`.  
```scala
import org.apache.spark.sql.SparkSession

object SimpleApp {
  def main(args: Array[String]) {
    val logFile = "/usr/local/spark/README.md"
    val spark = SparkSession.builder.appName("Simple Application").getOrCreate()
    val logData = spark.read.textFile(logFile).cache()
    val numAs = logData.filter(line => line.contains("a")).count()
    val numBs = logData.filter(line => line.contains("b")).count()
    println(s"Lines with a: $numAs, Lines with b: $numBs")
    spark.stop()
  }
}
```
You can see that a `SparkSession` instance is initialized as part of the program, which is unlike the Spark shell. A `SparkSession` instance is initialized automatically when launching the Spark shell.  
Thirdly, you can create a JAR package containding the application's code, then use the `spark-submit` to run your program.  
```
$ find .
$ sbt package
$ spark-submit --class "SimpleApp" target/scala-2.11/quickstart_2.11-0.1.jar
...
Lines with a: 61, Lines with b: 30
```

### Initializing Spark in Scala
```scala
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._

val conf = new SparkConf().setMaster("local").setAppName("My App")
val sc = new SparkContext(conf)
```

## Programming on RDDs
### Creating RDDs
```
val lines = sc.parallelize(List("pandas", "i like pandas"))
val lines = sc.textFile("/path/to/README.md")
```

### Transformations
filter(), map(), flatMap() transformations
```
val inputRDD = sc.textFile("log.txt")
val errorRDD = inputRDD.filter(line => line.contains("error"))
val warningRDD = inputRDD.filter(line => line.contains("warning"))
val result = input.map(x => x * x)
val words = lines.flatMap(lines => line.split(" "))
```
distinct(), union(), intersection(), subtract() set operations
```
badLinesRDD = errorRDD.union(warningRDD)
```

### Actions
`take(k)` returning k elements
```
badLineRDD.take(10).foreach(println)
```


