# Setting Up a Multi-node Cluster in Hadoop
## Background
* Two machines: one's OS is Mac OS Sierra 10.12.6, the other's is Ubuntu 16.04. And Hadoop 3.0.0 has been installed on both machines. The posts about installing Hadoop on a single-node can be found at:  
    - [Installing Hadoop on Mac OS X](../blog/b20171003.md)
    - [Installing Hadoop on Ubuntu 16.04](../blog/b20171009.md)
* The Hadoop installation directory should be on the same path for all machines.

## Networking
- To get the address of each machine:
```
$ ifconfig | grep inet
inet 127.0.0.1 netmask 0xff000000
inet6 ::1 prefixlen 128
inet6 fe80::1%lo0 prefixlen 64 scopeid 0x1
inet6 fe80::14cf:5e:34de:3658%en0 prefixlen 64 secured scopeid 0x4
inet 192.168.1.7 netmask 0xffffff00 broadcast 192.168.1.255
inet6 fe80::689d:f8ff:fe7e:3d0f%awdl0 prefixlen 64 scopeid 0x7
inet6 fe80::123e:ad93:bcfd:537b%utun0 prefixlen 64 scopeid 0x9
```
The Ethernet address here is `192.168.1.7`.

- `ping` other machines via IP address  
```
$ ping 192.168.1.7
PING 192.168.1.7 (192.168.1.7) 56(84) bytes of data.
64 bytes from 192.168.1.7: icmp_seq=1 ttl=64 time=88.2 ms
64 bytes from 192.168.1.7: icmp_seq=2 ttl=64 time=110 ms
64 bytes from 192.168.1.7: icmp_seq=3 ttl=64 time=28.6 ms
64 bytes from 192.168.1.7: icmp_seq=4 ttl=64 time=50.8 ms
^C
--- 192.168.1.7 ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3004ms
rtt min/avg/max/mdev = 28.637/69.606/110.679/31.870 ms
```
- `ping` other machines via host name  
To connect other machines via host name, firstly, you should make your current machine know the corresponding IP address and host name of the machine that you want to connect to.
```
$ sudo nano /etc/hosts
```
Append the following information in the `/etc/hosts` file for **each machine**.
```
192.168.1.7 <masterhostname>
192.168.1.14 <slavehostname>
```
Now you can test whether the slave machine can be connected from the master machine via the hostname, and vice versa.
```
$ ping <slavehostname>
```
- `ssh` other machines via username@hostname  

To access slave machine from master machine, you can use the command:
```
$ ssh <slaveusername>@<slavehostname>
```
You can logout using the command `exit` or `ctrl+d`.  
- Set up passowrd-less SSH access from your master machine to workers.
On master:
```
$ ssh-keygen -t rsa -P "" -f ~/.ssh/id_rsa
$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
```
On workers: copy *~/.ssh/id_rsa.pub* from your master to the worker 
```
$ ssh-copy-id -i ~/.ssh/id_rsa.pub <username>@<hostname>
$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
$ chmod 600 ~/.ssh/authorized_keys
```

## Configuration
On a single-node, Hadoop is often configured in Pseudo-distributed mode. To run Hadoop on a cluster of machines, it should be setup in a fully distributed mode.  
> **for both, master and slave**  
- *core-site.xml*
```xml
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://<masterhostname>/</value>
  </property>
  <property>
    <name>hadoop.tmp.dir</name>
    <value>/Users/${user.name}/hadoop/tmp</value>
  </property>
</configuration>
```  
**Note:** the property *hadoop.tmp.dir* is set as `/home/${user.name}/hadoop/tmp` for Ubuntu. The default value of *hadoop.tmp.dir* is `/tmp/hadoop-${user.name}`, but the files in `/tmp` will be cleaned up when the machine is restarted.
- *hdfs-site.xml*
```
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>2</value>
  </property>
</configuration>
```  
- *mapred-site.xml*
```
<configuration>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
</configuration>
```
- *yarn-site.xml*
```
<configuration>
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value><masterhostname></value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
</configuration>
```  
> **only for master**  
- *etc/hadoop/workers*
List all workers here:
```
# localhost
<slaveusername>@<slavehostname>
```
If you use one single user account for all machines, just list workers' hostnames here. The master machine can also be used as a datanode:
```
<masterusername>@<masterhostname>
<slaveusername>@<slavehostname>
```

## Format the filesystem
> **only for master**  
```
$ hdfs namenode -format
```  
A *dfs/name* folder will be created in the *hadoop.tmp.dir* directory.

## Start Hadoop
> **only for master**
```
sbin/start-dfs.sh
sbin/start-yarn.sh
```  
For the `start-dfs.sh` command, you can go to the web UI at *http://masterhostname-or-masterIP:9870* (the port is 50070 for earlier versions), and check the live nodes. For the `start-yarn.sh`, you can go to the address *http://masterhostname-or-masterIP:8088* to check the active nodes.  
**Debugging**: If the data node isn't running, you should remove the files in the *hadoop.tmp.dir* directory on the workers, and rerun `start-dfs.sh`.

Moreover, you can login the master machine, and
```
$ jps
4197 NameNode
4485 ResourceManager
4551 Jps
4347 SecondaryNameNode
```
Login the worker machine, and
```
$ jps
27682 DataNode
29140 NodeManager
29815 Jps
```
Or you can get the report of your cluster by the command:
```
$ hdfs dfsadmin -report
```
You can check the logs at the directory `HADOOP_HOME/logs`.

## Stop Hadoop
```
sbin/stop-yarn.sh
sbin/stop-dfs.sh
or 
sbin/stop-all.sh
```
