# Setting Up a Multi-node Cluster in Hadoop
## Background
  * Two machines: one's OS is Mac OS Sierra 10.12.6, the other's is Ubuntu 16.04.
    Hadoop 2.8.1 has been installed on both machines.
    The posts about installing Hadoop on a single-node can be found at:
    - [Installing Hadoop on Mac OS X via Homebrew](../blog/b20171003.md)
    - [Installing Hadoop on Ubuntu 16.04](../blog/b20171009.md)
  * It's better to set the same values to environment variables (e.g. HADOOP_HOME) for all machines.

## Networking
  - To get the IP address of each machine:
    ```
    $ ifconfig | grep inet
    inet 127.0.0.1 netmask 0xff000000
	  inet6 ::1 prefixlen 128
	  inet6 fe80::1%lo0 prefixlen 64 scopeid 0x1
	  inet6 fe80::14cf:5e:34de:3658%en0 prefixlen 64 secured scopeid 0x4
	  inet 192.168.1.7 netmask 0xffffff00 broadcast 192.168.1.255
	  inet6 fe80::689d:f8ff:fe7e:3d0f%awdl0 prefixlen 64 scopeid 0x7
	  inet6 fe80::123e:ad93:bcfd:537b%utun0 prefixlen 64 scopeid 0x9
    ```
    The IP address here is `192.168.1.7`.

  - `ping` other machines via IP address
    ```
    $ ping 192.168.1.7
    PING 192.168.1.7 (192.168.1.7) 56(84) bytes of data.
    64 bytes from 192.168.1.7: icmp_seq=1 ttl=64 time=88.2 ms
    64 bytes from 192.168.1.7: icmp_seq=2 ttl=64 time=110 ms
    64 bytes from 192.168.1.7: icmp_seq=3 ttl=64 time=28.6 ms
    64 bytes from 192.168.1.7: icmp_seq=4 ttl=64 time=50.8 ms
    ^C
    --- 192.168.1.7 ping statistics ---
    4 packets transmitted, 4 received, 0% packet loss, time 3004ms
    rtt min/avg/max/mdev = 28.637/69.606/110.679/31.870 ms
    ```
  - `ping` other machines via host name

    To connect other machines via host name, firstly, you should make your current machine know the corresponding IP address and host name of the machine that you want to connect to.
    ```
    $ sudo nano /etc/hosts
    ```
    Append the following information in the `/etc/hosts` file for **each machine**.
    ```
    192.168.1.7 <masterhostname>
    192.168.1.14 <slavehostname>
    ```
    Now you can test whether the slave machine can be connected from the master machine, and vice versa.
    ```
    $ ping <slavehostname>
    ```
  - `ssh` other machines via username@hostname

    To login slave machine from master machine, you can use the command:
    ```
    $ ssh <slaveusername>@<slavehostname>
    ```
    You can logout using the command `exit`.

## Configuration
  On a single-node, Hadoop is often configured in Pseudo-distributed mode. To run Hadoop on a cluster of machines, it should be setup in a fully distributed mode.
  > **for both, master and slave**

  - *core-site.xml*
    ```
    <configuration>
      <property>
        <name>fs.defaultFS</name>
        <value>hdfs://<masterhostname>/</value>
      </property>
    </configuration>
    ```
  - *hdfs-site.xml*
    ```
    <configuration>
      <property>
        <name>dfs.replication</name>
        <value>2</value>
      </property>
      <property>
        <name>dfs.name.dir</name>
        <value>/usr/local/Cellar/hadoop/dfs/name</value>
      </property>
      <property>
        <name>dfs.data.dir</name>
        <value>/usr/local/Cellar/hadoop/dfs/data</value>
      </property>
    </configuration>
    ```
    If master machine is used as namenode and datanode, set up the `dfs.name.dir` and `dfs.data.dir` on it. Otherwise, that is master machine is used as namenode, only set up the `dfs.name.dir`. For the slave machine, only `dfs.data.dir` is set up.
  - *yarn-site.xml*
    ```
    <configuration>
      <property>
        <name>yarn.resourcemanager.hostname</name>
        <value><masterhostname></value>
      </property>
      <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
      </property>
    </configuration>
    ```
  - *mapred-site.xml*
    ```
    <configuration>
      <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
      </property>
    </configuration>
    ```
  > **only for master**
  
  - *etc/hadoop/masters*
    ```
    <masterusername>@<masterhostname>
    ```
  - *etc/hadoop/slaves*
    If the master machine is only used for namenode, you should only list slave machines here.
    ```
    <slaveusername>@<slavehostname>
    ```
    If the master machine is also used for datanode, you need to list all the master and slave machines here.
    ```
    <masterusername>@<masterhostname>
    <slaveusername>@<slavehostname>
    ```

## Format the filesystem
  > **only for master**
  ```
  $ hdfs namenode -format
  ```

  The default storage directory is `/tmp/hadoop-<masterusername>/dfs/`.

## Start Hadoop
  > **only for master**
  ```
  sbin/start-dfs.sh
  sbin/start-yarn.sh
  ```

  To check whether the multi-node cluster is set up properly, login the master machine, and
  ```
  $ jps
  4197 NameNode
  4485 ResourceManager
  4551 Jps
  4347 SecondaryNameNode
  ```
  Login the slave machine, and
  ```
  $ jps
  27682 DataNode
  29140 NodeManager
  29815 Jps
  ```
  Go to the web UI http://masterhostname-or-masterIP:8088 and http://masterhostname-or-masterIP:50070. Check the number of active/live nodes. Or you can get the report of your cluster by the command:
  ```
  $ hdfs dfsadmin -report
  ```
  You can check the logs at the directory `HADOOP_HOME/logs`.

## Stop Hadoop
  ```
  sbin/stop-yarn.sh
  sbin/stop-dfs.sh
  ```
