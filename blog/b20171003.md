# Installing Hadoop on Mac OS X
## Prerequisites
- Java
```
$ java -version
java version "1.8.0_161"
Java(TM) SE Runtime Environment (build 1.8.0_161-b12)
Java HotSpot(TM) 64-Bit Server VM (build 25.161-b12, mixed mode)
```
If you'd like to upgrade your JDK, you should firstly uninstall JDK from your system. Then download the latest release JDK installer from http://www.oracle.com/technetwork/java/javase/downloads/index.html.  
Follow the lines below to uninstall old-version JDK:
```
$ sudo rm -fr /Library/Internet\ Plug-Ins/JavaApplet/Plugin.plugin
$ sudo rm -fr /Library/PreferencePanes/JavaControlPanel.prefPane
$ sudo rm -fr ~/Library/Application\ Support/Java
$ cd /Library/Java/JavaVirtualMachines
$ sudo rm -fr jdk1.8.0_144.jdk
```
- Ruby & Homebrew
```
$ ruby -e "$(curl -fsSL https://raw.github.com/Homebrew/homebrew/go/install)"
```
- SSH
Check for the existance of `~/.ssh/id_rsa` and `~/.ssh/id_rsa.pub` files.
If not, the keys can be generated by
```
ssh-keygen -t rsa
```
- Enable Remote Login
"System Preferences" -> "Sharing" -> Check "Remote Login"
- Authorize SSH keys
```
$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
```

## Install Hadoop
There are two ways to install Hadoop on Mac OS X: 
1. Installing Hadoop via Homebrew: the default installation directory (i.e. HADOOP_HOME) is */usr/local/Cellar/hadoop/3.0.0/libexec*.
```
$ brew install Hadoop
```
2. Downloading the latest Binary release of Hadoop from http://hadoop.apache.org/releases.html, untar the tarball file and move it in a proper directory. Here the installation directory is */usr/local/hadoop*
```
$ tar -xzf Hadoop-3.0.0.tar.gz
$ sudo mv Hadoop-3.0.0 /usr/local
$ cd /usr/local
$ sudo mv Hadoop-3.0.0 hadoop
```

## Configure Hadoop
Hadoop can be run in one of the three modes: Standalone(local) mode, Pseudodistributed mode, Fully distributed mode. The configuration files for the Pseudodistributed mode is provided here.
- Go to Hadoop configuration directory (i.e. HADOOP_CONF_DIR):
```
$ cd $HADOOP_HOME/etc/hadoop
```
- Edit *hadoop-env.sh* (This is not necessary for Hadoop 3.0.0)
Replace
```
export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true"
```
with
```
export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true -Djava.security.krb5.realm= -Djava.security.krb5.kdc="
```
- Edit *core-site.xml*
```xml
<configuration>  
  <property>
    <name>hadoop.tmp.dir</name>
    <value>/Users/${user.name}/hadoop/tmp</value>
  </property>
  <property>
    <name>fs.defaultFS</name>                                     
    <value>hdfs://localhost/</value>                             
  </property>
</configuration>
```
- Edit *hdfs-site.xml*
```
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>
</configuration>
```
- Edit *mapred-site.xml*
```
<configuration>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
</configuration>
```
- Edit *yarn-site.xml*
```
<configuration>
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>localhost</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
</configuration>
```

## Start Hadoop
For the sake of simplicity, add the Hadoop installation directory to the environment and add the *bin* and *sbin* directories to the path through putting the following lines into *~/.bashrc* file.
```
export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
$ source ~/.bashrc
```
- Format HDFS
```
$ hdfs namenode -format
```
- Run Hadoop
```
$ start-dfs.sh
$ start-yarn.sh
```
or
```
$ start-all.sh
```
or create an alias for a set of actions in the *~/.profile* file.
```
alias hstart="/usr/local/hadoop/sbin/start-dfs.sh;/usr/local/hadoop/sbin/start-yarn.sh"
alias hstop="/usr/local/hadoop/sbin/stop-yarn.sh;/usr/local/hadoop/sbin/stop-dfs.sh"
$ source ~/.profile
$ hstart
```
If you get the warning:  
> WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable  
It is because you are running on 64-bit but Hadoop native library is 32-bit.

## Testing
- Access Hadoop web UI  
> - NameNode status: http://localhost:9870 (in earlier versions, the port is 50070)
> - Secondary NameNode status: http://localhost:50090
> - Resource Manager page: http://localhost:8088

- Check status
```
$ jps
13712 NameNode
12678 DataNode
12920 ResourceManager
12793 SecondaryNameNode
14042 Jps
13023 NodeManager
```
- Run a MapReduce job
```
$ hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar pi 2 5
Number of Maps = 2
Samples per Map = 5
...
Job Finished in 25.288 seconds
Estimated value of Pi is 3.6000000000000000
```

## Stop Hadoop  
```
$ stop-yarn
$ stop-dfs
or
$ stop-all
or
$ hstop
```

# Uninstall Hadoop
If you'd like to upgrade your Hadoop, you have to uninstall the old version beforehand. If you installed the old version via Homebrew, you can also uninstall it with Homebrew.
```
$ brew remove --force hadoop
```
If you've used the binary release, you can just delete the installation directory from your system.
