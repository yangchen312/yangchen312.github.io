# Get started with Spark  
Apache Spark is a cluster computing framework for large-scale data processing. It doesn't use MapReduce as an execution engine, instead, it uses its own distributed runtime for executing work on a cluster.  
## Installing Spark  
 - Download a release of the Spark binary distribution from the website https://spark.apache.org/downloads.html.  
 - Unpack the tarball in a suitable directory, like `/usr/local`, and rename the it.
 - Put the Spark binaries on your path.  
 ```
 export SPARK_HOME=/usr/local/spark
 export PATH=$PATH:$SPARK_HOME/bin
 ```

## spark-shell
It's an interactive session of Spark, which is a Scala REPL with a few Spark additions. Spark web UI available at http://namenode:4040
```
$ spark-shell
...
Spark context available as 'sc'.
...
scala>
```
The shell has created a Scala variable, `sc`, to store the SparkContext instance.  
To set which master the context connects to and add `code.jar` to its classpath, use:
```
$ spark-shell --master local[*] --jars code.jar
```
To include a dependency, use:
```
$ spark-shell --master local[2] --packages "org.example:example:0.1"
```

### An example  
```
scala> var lines = sc.textFile("input/ncdc/micro-tab/sample.txt")
lines: org.apache.spark.rdd.RDD[String] = input/ncdc/micro-tab/sample.txt MapPartitionsRDD[1] at textFile at <console>:24
scala> var records = lines.map(_.split("\t"))
records: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[2] at map at <console>:26
scala> var filtered = records.filter(rec => (rec(1) != "9999" && rec(2).matches("[01459]")))
filtered: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[3] at filter at <console>:28
scala> var tuples = filtered.map(rec => (rec(0).toInt, rec(1).toInt))
tuples: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[4] at map at <console>:30
scala> var maxTemps = tuples.reduceByKey((a, b) => Math.max(a, b))
maxTemps: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[5] at reduceByKey at <console>:32
scala> maxTemps.foreach(println(_))
scala> maxTemps.saveAsTextFile("output")
```
Please note that the input and output are local rather than HDFS directories. You can quit the spark shell using `:quit`.  
To make the logging less verbose, you can set up the below property in the `conf/log4j.properties` file.
```
log4j.rootCategory=INFO, console
```
To lower the log level, use this:
```
log4j.rootCategory=WARN, console
```

## pyspark
Spark can be run with Python in interactive mode using `pyspark` command.
```
$ pyspark
>>>
```
You can exit using `quit()` or `Ctrl+D`.
