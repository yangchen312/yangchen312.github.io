# Get Started with MLlib
MLlib contains only parallel algorithms that run well on clusters, which is best suited for running algorithms on a large dataset. If you have many small datasets, you'd better use a single node learning library, e.g. Weka or Scikit-Learn on each node before using a Spark *map()*.

## An Example: Spam Classification  
This example is from the "Learning Spark" book. The ML algorithm "LogisticRegressionWithSGD" works on RDDs, but it is deprecated in Spark 2.0.0.  
```python
from pyspark import SparkConf, SparkContext
from pyspark.mllib.feature import HashingTF
from pyspark.mllib.regression import LabeledPoint
from pyspark.mllib.classification import LogisticRegressionWithSGD

# Import data
conf = SparkConf().setAppName("Spam_Classification").setMaster("local[*]")
sc = SparkContext(conf=conf)
spam = sc.textFile("spam.txt")
ham = sc.textFile("ham.txt")

# Preprocessing
tf = HashingTF(numFeatures=10000)
spamFeatures = spam.map(lambda email: tf.transform(email.split(" ")))
hamFeatures = ham.map(lambda email: tf.transform(email.split(" ")))
positiveFeatures = spamFeatures.map(lambda features: LabeledPoint(1, features))
negativeFeatures = hamFeatures.map(lambda features: LabeledPoint(0, features))
trainingData = positiveFeatures.union(negativeFeatures)
trainingData.cache()

# Train a model
model = LogisticRegressionWithSGD.train(trainingData)

# Model evaluation
posTest = tf.transform("O M G GET cheap stuff by sending money to ...".split(" "))
negTest = tf.transform("Hi Dad, I started studying Spark the other ...".split(" "))
print model.predict(posTest)
print model.predict(negTest)
```
You can run the code in an IDE, e.g. PyCharm, or on submit it to spark-submit.
```
$ cd project-folder
$ spark-submit spam.py
```


