# Running Spark on a Cluster
Spark depends on a cluster manager to launch executors, and in certain cases, to launch the driver. It can run on top of different external managers, like YARN, Mesos, and its built-in Stand-alone cluster manager.

## Running Spark on YARN
### spark-submit
- Set the Spark's environment variables `HADOOP_CONF_DIR` or `YARN_CONF_DIR` in the `conf/spark-env.sh` file.
```
export HADOOP_CONF_DIR=/usr/local/Cellar/hadoop/2.8.1/libexec/etc/hadoop
# or
export YARN_CONF_DIR=/usr/local/Cellar/hadoop/2.8.1/libexec/etc/hadoop
```
- Launch a Spark application in distributed mode.
```
$ spark-submit --class examples.SimpleApp --master yarn --deploy-mode cluster target/simple-project-1.0-SNAPSHOT.jar
$ spark-submit --master yarn --deploy-mode client SimpleApp.py
```
You can set the `--master` flag with values  `local`, `local[n]` and `local[*]` for the local mode; `yarn`, `mesos://host:port` and `spark://host:port` for the distributed mode. The `--deploy-mode` flag indicates whether to launch the driver program locally (`client`) or on one of the worker machines inside the cluster (`cluster`).  
Note that in the application the default filesystem is HDFS.


## Spark's Standalone Cluster Manager
### Launch the Standalone Cluster Manager
- Install spark on all your machines at the same location, e.g. `/usr/local/spark`.
- Ensure that your machines can access each other using `ssh`.
- Setting the conf/slaves file on your master, and fill in the workers' hostnames.
- To start the cluster, run `sbin/start-all.sh` on your master. You can check all your workers and submitted applications on the cluster manager's web UI `http://masternode:8080`.

### Run applications using spark-submit
```
$ spark-submit --master spark://masternode:7077 SimpleApp.py
$ spark-submit --class examples.SimpleApp --master spark://masternode:7077 --deploy-mode client target/simple-project-1.0-SNAPSHOT.jar
```
You can also launch `spark-shell` or `pyspark` on the cluster by passing the `--master` flag.
```
$ spark-shell --master spark://masterndoe:7077
$ pyspark --master spark://masternode:7077
```










