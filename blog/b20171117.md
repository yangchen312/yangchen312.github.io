# Running Spark on a Cluster
Spark depends on a cluster manager to launch executors, and in certain cases, to launch the driver. It can run on top of different external managers, like YARN, Mesos, and its built-in Stand-alone cluster manager.

## Spark's Standalone Cluster Manager
### Launch the Standalone Cluster Manager
- Install spark on all your machines at the same location, e.g. `/usr/local/spark`.
- Ensure that your machines can access each other using `ssh`.
- Setting the conf/slaves file on your master, and fill in the workers' hostnames.
- To start the cluster, run `sbin/start-all.sh` on your master. You can check all your workers and submitted applications on the cluster manager's web UI `http://masternode:8080`.
- To stop the cluster, run `sbin/stop-all.sh` on your master.

You can also launch the master and workers manually, using the `spark-class` script in Spark's *bin/* directory.
On your master,
```
$ bin/spark-class org.apache.spark.deploy.master.Master
```
You can access the cluster manager's web UI *http://masternode:8080*, where no worker is alive. Then on workers,
```
$ bin/spark-class org.apache.spark.deploy.worker.Worker spark://masternode:7077
```
You will see the corresponding workers appear on the cluster manager's web UI.

### Run Spark Applications in the Standalone Cluster mode
```
$ spark-submit --master spark://masternode:7077 SimpleApp.py
$ spark-submit --class examples.SimpleApp --master spark://masternode:7077 --deploy-mode client target/simple-project-1.0-SNAPSHOT.jar
```
You can also launch `spark-shell` or `pyspark` on the cluster by passing the `--master` flag value.
```
$ spark-shell --master spark://masterndoe:7077
$ pyspark --master spark://masternode:7077
```
In practice, when running Spark on a cluster, typically you pass `master` flag to `spark-submit` to launching an application. However, for local testing and unit tests, you can hardcode `master` in the program.


## Running Spark on YARN
The steps to run Spark on YARN is similar to those for the Standalone Cluster mode. Some additional configurations are required:  
- Start HDFS and YARN before starting Spark
Run `$HADOOP_HOME/sbin/start-dfs.sh` and `$HADOOP_HOME/sbin/start-yarn.sh`, check whether they are correctly started with `jps`, and the NameNode and ResourceMAnager web UI: *http://namenode:9870* and *http://namenode:8088*.  

- Set the Spark's environment variables *HADOOP_CONF_DIR* or *YARN_CONF_DIR* in the *conf/spark-env.sh* or *~/.bashrc* file.
```
export HADOOP_CONF_DIR or YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop
```

- Set either *spark.yarn.jars* or *spark.yarn.archive* in the *conf/spark-defaults.conf* file.
Two ways available: 
1. Create the archive: `$ jar cv0f spark-libs.jar -C $SPARK_HOME/jars/ .`
2. Upload it into HDFS: `$ hdfs dfs -put spark-libs.jar /spark/` (You should create the directory */spark* beforehand)
3. Append the following line into the file `spark.yarn.archive hdfs:///spark/spark-libs.jar`.  
Or  
1. In the *SPARK_HOME/jars* directory, create the archive containing all the JARs: `$ zip spark-archive.zip *`.
2. Upload it into HDFS: `$ hdfs dfs -put spark-archive.zip /user/${user.name}/`.
3. Append the following line into the file `spark.yarn.archive hdfs:///user/${user.name}/spark-archive.zip`.
**Debugging:** If you haven't set these variables correctly, a spark exception would occur, i.e. *ERROR SparkContext: Error initializing SparkContext. org.apache.spark.SparkException: Yarn application has already ended!*

- If you meet a *java.nio.channels.ClosedChannelException: ERROR client.TransportClient: Failed to sent RPC*, you should set the properties below as *false* in the *yarn-site.xml* file.
```xml
<property>
  <name>yarn.nodemanager.pmem-check-enabled</name>
  <value>false</value>
</property>
<property>
  <name>yarn.nodemanager.vmem-check-enabled</name>
  <value>false</value>
</property>
``` 

### Run Spark Applications On YARN
```
$ spark-shell --master yarn --deploy-mode client
$ pyspark --master yarn --deploy-mode client
$ spark-submit --class examples.SimpleApp --master yarn --deploy-mode cluster target/simple-project-1.0-SNAPSHOT.jar
$ spark-submit --master yarn --deploy-mode client SimpleApp.py
```
You can set the `--master` flag with values  `local`, `local[n]` and `local[*]` for the local mode; `yarn`, `mesos://host:port` and `spark://host:port` for the distributed mode. The `--deploy-mode` flag indicates whether to launch the driver program locally (`client`) or on one of the worker machines inside the cluster (`cluster`).  
Note that in the application the default filesystem is HDFS.




