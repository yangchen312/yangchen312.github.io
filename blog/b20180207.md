# Get Started with Spark Streaming
Spark Streaming is a Spark's module for the applications that act on data as soon as it arrives. It provides an abstraction call DStreams, or discretized streams, which is a sequence of RDDs arriving at each time step. Spark Streaming ships as a separate Maven artifact and has some additional imports you will want to add to your project.  
- For a Maven project
```
<dependency>
	<groupId>org.apache.spark</groupId>
	<artifactId>spark-streaming_2.11</artifactId>
	<version>2.2.1</version>
</dependency>
```
- For a SBT project
```
libraryDependencies += "org.apache.spark" % "spark-streaming_2.11" % "2.2.1"
```

- An example
Receive a stream of newline-delimited lines of text from a server running at port 7777, filter only the lines that contain the word *error*, and print them.  
1. In Scala
```scala
import org.apache.spark.SparkConf
import org.apache.spark.streaming.{Seconds, StreamingContext}

object ErrorLines extends App {
  val conf = new SparkConf().setAppName("ErrorLines").setMaster("local[2]")
  // Create a StreamingContext with a 1-second batch size from a SparkConf
  val ssc = new StreamingContext(conf, Seconds(1))
  // Create a DStream using data received after connecting to port 7777 on the local machine
  val lines = ssc.socketTextStream("localhost", 7777)
  // Filter our DStream for lines with "error"
  val errorLines = lines.filter(_.contains("error"))
  // Print out the lines with errors
  errorLines.print()
  
  // Start our streaming context
  ssc.start()
  // Wait for the job to finish
  ssc.awaitTermination()
}
```
You will first need to run NetCat as a data server:
```
$ nc -lk 7777
```
Then, you can run the project using IDE, e.g. Intellij. Or run it in SBT CLT.
```
$ cd /path/to/project_folder
$ sbt
sbt:project_name> run
```
You can enter some lines of text in the terminal of the data server. Then in the SBT CLT, you will see that the lines with the word "error" are printed. Note that a streaming context can be started only once, and must be started after we set up all the DStreams and output operations we want. Moreover, do not run Spark Streaming locally with master configured as "local" or "local[1]", since this allocates only one CPU for tasks and if a receiver is running on it, there is no resource left to process the received data. Use at least "local[2]" to have more cores.


2. In Java
```java
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;

public class ErrorLines{
	public static void main(String[] args) {
		SparkConf conf = new SparkConf().setMaster("local[2]").setAppName("ErrorLines");
		JavaStreamingContext jssc = new JavaStreamingContext(conf, Durations.seconds(1));
		JavaDStream<String> lines = jssc.socketTextStream("localhost", 7777);
		JavaDStream<String> errorLines = lines.filter(new Function<String, Boolean>() {
			public Boolean call(String line) {
				return line.contains("error");
			}
		});
		errorLines.print();
		
		jssc.start();
		try{
			jssc.awaitTermination();
		} catch (InterruptedException e) {
			System.out.println("Got interrupted!");
		}
	}
}
```
As with Scala, you can run this project with IDE, e.g. Intellij. Or package it as a Spark application, and submit it to *spark-submit*.
```
$ cd project_folder
$ mvn package
$ spark-submit --class ErrorLines target/learningspark-1.0.jar
```
**Debugging:** if the jar file is not executable, you should append the jar plugin in the Maven project's pom.xml file.
```xml
<build>
  <plugins>
    <plugin>
      <groupId>org.apache.maven.plugins</groupId>
      <artifactId>maven-jar-plugin</artifactId>
      <version>3.0.2</version>
      <configuration>
        <archive>
          <manifest>
            <mainClass>ErrorLines</mainClass>
          </manifest>
        </archive>
      </configuration>
    </plugin>
  </plugins>
</build>
```




