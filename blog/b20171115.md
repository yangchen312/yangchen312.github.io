# PageRank Algorithm
## Main Idea
It aims to assign a measure of importance ("rank") to each document in a set based on how many documents have links to it.  

## Data  
Two data sets are required for this algorithm:  
>- One of (pageID, linkList) elements containing the list of neighbors of each page
>- One of (pageID, rank) elements containing the current rank for each page.

## Algorithm
>1. Initialize each page's rank to 1.0.
>2. On each iteration, have page `p` send a contribution of `rank(p)/numNeighbors(p)` to its neighbors (the pages it has links to).
>3. Set each page's rank to `0.15 + 0.85 * contributionsReceived`.  
The last 2 steps repeat for several iterations, in practice, it's typical to run about 10 iterations. The algorithm will converge to the correct PageRank value for each page.

## Spark in Scala Implementation 
Suppose that the links of a page is stored as `x y,a,c,e` in each line, where `x` is pageId and `y,a,c,e` are neighbors.
```
val textFile = sc.textFile("links.txt")
val lines = textFile.map(line => (line.split(" ")(0), line.split(" ")(1).split(",")))
val links = lines.partitionBy(new org.apache.spark.HashPartitioner(2)).persist()
var ranks = links.mapValues(v => 1.0)
for (i <- 0 until 10) {
	val contributions = links.join(ranks).flatMap{
							case (pageId, (links, rank)) =>
							links.map(dest => (dest, rank / links.size))
	}
	ranks = contributions.reduceByKey((x, y) => x + y).mapValues(v => 0.15 + 0.85 * v)
}
ranks.foreach(println)
```