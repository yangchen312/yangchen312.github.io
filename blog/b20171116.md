# Loading and Saving Data in Spark
There are three common sets of data sources for Spark and its ecosystem.
- File formats and filesystems
- Structured data source through Spark SQL
- Databases and key/value stores

## File Formats
### Text Files
Scala code  
```scala
val input = sc.textFile("/file/path/filename")
val input = sc.wholeTextFile("/files/directory")
```
Python code  
```python
input = sc.textFile("/file/path/filename")

result.saveAsTextFile("/path/filename")
```
Java code
```java
JavaRDD<String> input = sc.textFile("/file/path/filename")
```

### Json Data
Scala code  
```scala
import com.fasterxml.jackson.module.scala.DefaultScalaModule
import com.fasterxml.jackson.module.scala.experimental.ScalaObjectMapper
import com.fasterxml.jackson.databind.ObjectMapper
import com.fasterxml.jackson.databind.DeserializationFeature
case class Person(name: String, lovesPandas: Boolean)
val result = input.flatMap(record => {
	try {
		Some(mapper.readValue(record, classOf[Person]))
	} catch {
		case e: Exception => None
	}
})

result.filter(p => p.lovesPandas).map(mapper.writeValueAsString(_)).saveAsTextFile("/file/path/filename")
```
Python code  
```python
import json
data = input.map(lambda x: json.loads(x))

data.filter(lambda x: x['lovesPandas']).map(lambda x: json.dumps(x)).saveAsTextFile("/file/path/filename")
```

### CSV/TSV Files
Scala code
```scala
import Java.io.StringReader
import au.com.bytecode.opencsv.CSVReader
val input = sc.textFile("/file/path/filename")
val result = input.map{line => 
	val reader = new CSVReader(new StringReader(line));
	reader.readNext();
}

pandaLovers.map(person => List(person.name, persion.favoriteAnimal).toArray)
.mapPartitions{people =>
	val stringWriter = new StringWriter();
	val csvWriter = new CSVWriter(stringWriter);
	csvWriter.writeAll(people.toList)
	Iterator(stringWriter.toString)
}.saveAsTextFile("/file/path/filename")
```
Python code
```python
import csv
import StringIO
def loadRecord(line):
	"""Parse a CSV line"""
	input = StringIO.StringIO(line)
	reader = csv.DictReader(input, fieldnames = ["name", "favoriteAnimal"])
	return reader.next()
input = sc.textFile("/file/path/filename").map(loadRecord)

def writeRecords(records):
	"""Write out CSV lines"""
	output = StringIO.StringIO()
	writer = csv.DictWriter(output, fieldnames = ["name", "favoriteAnimal"])
	for record in records:
		writer.writerow(record)
	return [output.getvalue()]
pandaLovers.mapPartitions(writeRecords).saveAsTextFile("/file/path/filename")
```

### SequenceFiles
Scala code
```scala
val data = sc.sequenceFile(inFile, classOf[Text], classOf[IntWritable]).map{case (x, y) => (x.toString, y.get())}
data.saveAsSequenceFile(outFile)
```
Python code
```python
data = sc.sequenceFile(inFile, "org.apache.hadoop.io.Text", "org.apache.hadoop.io.IntWritable")
```

## Spark SQL
### Apache Hive
Scala code
```scala
import org.apache.spark.sql.hive.HiveContext

val hiveCtx = new org.apache.spark.sql.hive.HiveContext(sc)
val rows = hiveCtx.sql("SELECT name, age FROM users")
val firstRow = rows.first()
printlin(firstRow.getString(0))

/* Loading Json data */
var tweets = hiveCtx.jsonFile('tweets.json')
tweets.registerTempTable("tweets")
val results = hiveCtx.sql("SELECT user.name, text FROM tweets")
```
Python code
```python
from pyspark.sql import HiveContext

hiveCtx = HiveContext(sc)
rows = hiveCtx.sql("SELECT name, age FROM users")
firstRow = row.first()
print firstRow.name

"""Loading Json data"""
tweets = hiveCtx.jsonFile("tweets.json")
tweets.registerTempTable("tweets")
results = hiveCtx.sql("SELECT user.name, text FROM tweets")
```
Java code
```java
import org.apache.spark.sql.hive.HiveContext;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SchemaRDD;

HiveContext hiveCtx = new HiveContext(sc);
SchemaRDD rows = hiveCtx.sql("SELECT name, age FROM users");
Row firstRow = rows.first();
System.out.println(firstRow.getString(0));

/* Loading Json data */
SchemaRDD tweets = hiveCtx.jsonFile("tweets.json");
tweets.registerTempTable("tweets");
SchemaRDD results = hiveCtx.sql("SELECT user.name, text FROM tweets");
```

## Databases
### Java Database Connectivity
Spark can load data from any relational database that supports Java Database Connectivity (JDBC). Let's look into an example for MySQL in Scala.
```scala
import org.apache.spark.rdd.jdbcRDD
import java.sql.ResultSet

def createConnection() = {
	Class.forName("com.mysql.jdbc.Driver").newInstance();
	DriverManager.getConnection("jdbc:mysql://localhost/test?user=holden");
}

def extractValues(r: ResultSet) = {
	(r.getInt(1), r.getString(2))
}

val data = new JdbcRDD(sc, 
	createConnection, "SELECT * FROM panda WHERE ? <= id AND id <= ?", 
	lowerBound =1, upperBound = 3, numPartitions =2, mapRow = extractValues)
println(data.collect().toList)
```

### Cassandra
- Scala code  
sbt requirements for Cassandra connector
```
"com.datastax.spark" %% "spark-cassandra-connector" % "1.0.0-rc5"
"com.datastax.spark" %% "spark-cassandra-connector-java" % "1.0.0-rc5"
```
Setting the Cassandra property in Scala
```scala
val conf = new SparkConf(true).set("spark.cassandra.connection.host", "hostname")
val sc = new SparkContext(conf)
```
Loading data
```scala
import com.datastax.spark.connector._

// CREAT TABLE test.kv(key text PRIMARY KEY, value int);
val data = sc.cassandraTable("test", "kv")
data.map(row => row.getInt("value")).stats()
```
Saving data
```scala
val rdd = sc.parallelize(List(Seq("moremagic", 1)))
rdd.saveToCassandra("test", "kv", SomeColumns("key", "value"))
```

- Java code  
Maven requirements for Cassandra connector
```xml
<dependency>
	<groupId>com.datastax.spark</groupId>
	<artifactId>spark-cassandra-connector</artifactId>
	<version>1.0.0-rc5</version>
</dependency>
<dependency>
	<groupId>com.datasatx.spark</groupId>
	<artifactId>spark-cassandra-connector-java</artifactId>
	<version>1.0.0-rc5</version>
</dependency>
```
Setting the Cassandra property in Java
```java
SparkConf conf = new SparkConf(true).set("spark.cassandra.connection.host", cassandraHost);
JavaSparkContext sc = new JavaSparkContext(sparkMaster, "basicquerycassandra", conf);
```
Loading Data
```java
import com.datastax.spark.connector.CassandraRow;
import static com.datastax.spark.connector.CassandraJavaUtil.javaFunctions;

JavaRDD<CassandraRow> data = javaFunctions(sc).cassandraTable("test", "kv");
System.out.println(data.mapToDouble(new DoubleFunction<CassandraRow>() {
	public double call(CassandraRow row) {return row.getInt("value"); }
}).stats());
```

### HBase
```scala
import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.Result
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.mapreduce.TableInputFormat

val conf = HBaseConfiguration.create()
conf.set(TableInputFormat.INPUT_TABLE, "tablename")
val rdd = sc.newAPIHadoopRDD(conf, classOf[TableInputFormat], classOf[ImmutableBytesWritable], classOf[Result])
```

### Elasticsearch
```scala
/* Input */
def mapWritableToInput(in: MapWritable): Map[String, Stirng] = {
	in.map{case (k, v) => (k.toString, v.toString)}.toMap
}
val jobConf = new JobConf(sc.hadoopConfiguration)
jobConf.set(ConfigurationOptions.ES_RESOURCE_READ, args(1))
jobConf.set(ConfigurationOptions.ES_NODES, args(2))
val currentTweets = sc.hadoopRDD(jobConf,
	classOf[EsInputFormat[Object, MapWritable]], classOf[Object],
	classOf[MapWritable])
val tweets = currentTweets.map{case(key, value) => mapWritableToInput(value)}

/* Output */
val jobConf = new JobConf(sc.hadoopConfiguration)
jobConf.set("mapred.output.format.class", "org.elasticsearch.hadoop.mr.EsOutputFormat")
jobConf.setOutputCommitter(classOf[FileOutputCommitter])
jobConf.set(ConfigurationOptions.ES_RESOURCE_WRITE, "twitter/tweets")
jobConf.set(ConfigurationOptions.ES_NODES, "localhost")
FileOutputFormat.setOutputPath(jobConf, new Path("-"))
output.saveAsHadoopDataset(jobConf)
```


