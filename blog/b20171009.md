# Installing Hadoop on Ubuntu 16.04
## Prerequisites
- Java
```
$ sudo add-apt-repository ppa:webupd8team/java
$ sudo apt-get update
$ sudo apt-get install oracle-java8-installer
$ java -version
java version "1.8.0_144"
Java(TM) SE Runtime Environment (build 1.8.0_144-b01)
Java HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode)
```
- ssh

First, install ssh and check whether you can connect to the localhost without a passphrase.
```
$ sudo apt-get install ssh
$ ssh localhost
```
If you cannot connect to localhost without a passphrase, execute the following commands:
```
$ ssh-keygen -t rsa -P "" -f ~/.ssh/id_rsa
$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
$ chmod 0600 ~/.ssh/authorized_keys
```

## Install Hadoop
Visit the Apache Hadoop Releases page to find the most recent stable release,
and copy the download link for the release binary. On the server, we use `wget` to fetch it.
```
$ wget http://www-eu.apache.org/dist/hadoop/common/hadoop-2.8.1/hadoop-2.8.1.tar.gz
```
Then extract files, and move them into `/usr/local`.
```
$ tar -xzvf hadoop-2.8.1.tar.gz
$ sudo mv hadoop-2.8.1 /usr/local/hadoop
```

## Prepare to start Hadoop
To find the default Java path:
```
$ which java
/usr/bin/java
$ readlink -f /usr/bin/java | sed "s:bin/java::"
/usr/lib/jvm/java-8-oracle/jre/
```
Open the file `hadoop-env.sh`, then set the value of `JAVA_HOME`.
```
$ sudo nano /usr/local/hadoop/etc/hadoop/hadoop-env.sh
#export JAVA_HOME=${JAVA_HOME}
export JAVA_HOME=$(readlink -f /usr/bin/java | sed "s:bin/java::")
```
Try the following command:
```
$ cd /usr/local/hadoop
$ bin/hadoop
```
For the sake of convenience, we can create an environment variable that points to the Hadoop installation directory.
```
$ export HADOOP_HOME=/usr/local/hadoop
$ export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
```
These settings can be included in the shell startup file, such as ~/.bashrc or ~/.profile. These setting aren't applied for the post.

## Configuration
Pseudo-distributed mode
- *etc/hadoop/core-site.xml*
```
<configuration>  
  <property>
     <name>fs.defaultFS</name>                                     
     <value>hdfs://localhost/</value>                             
  </property>
</configuration>
```
- *etc/hadoop/hdfs-site.xml*
```
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>
</configuration>
```
- *etc/hadoop/mapred-site.xml*
```
<configuration>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
</configuration>
```
- *etc/hadoop/yarn-site.xml*
```
<configuration>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
</configuration>
```

## Start Hadoop
- To format the filesystem:
```
$ bin/hdfs namenode -format
```
*Debugging*: after executing `start-dfs` and `jps`, there is no DataNode running.
The reason might be the old data haven't been removed. So go to hadoop.tmp.dir,
and try this command `$ rm -rf *`, and then format filesystem again.

- To start NameNode daemon and DataNode daemon:
```
$ sbin/start-dfs.sh
```
Check the web UI for the *NameNode*, by default it's available at: `http://localhost:50070`.
Also run:
```
$ jps
13456 Jps
12873 NameNode
13276 SecondaryNameNode
13052 DataNode
```

- To create HDFS directories and run a MapReduce job:
```
$ bin/hdfs dfs -mkdir -p /user/<username>
$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar pi 2 5
```

- To start ResourceManager daemon and NodeManager daemon:
```
$ sbin/start-yarn.sh
```
Check the web UI for *ResourceManager*, by default it's available at: `http://localhost:8088`.
We can also run 'jps' and mapreduce jobs.

- To stop daemons:
```
$ sbin/stop-yarn.sh
$ sbin/stop-dfs.sh
```
